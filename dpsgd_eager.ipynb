{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019, The TensorFlow Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Training a CNN on MNIST in TF Eager mode with DP-SGD optimizer.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "\n",
    "GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "dpsgd = True\n",
    "learning_rate = 0.15\n",
    "noise_multiplier = 1.1\n",
    "l2_norm_clip = 1.0\n",
    "batch_size = 250\n",
    "epochs = 60\n",
    "microbatches =250\n",
    "\n",
    "def compute_epsilon(steps):\n",
    "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "  if noise_multiplier == 0.0:\n",
    "    return float('inf')\n",
    "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "  sampling_probability = batch_size / 60000\n",
    "  rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=noise_multiplier,\n",
    "                    steps=steps,\n",
    "                    orders=orders)\n",
    "  # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if dpsgd and batch_size % microbatches != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "  # Fetch the mnist data\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_images, train_labels = train\n",
    "  test_images, test_labels = test\n",
    "\n",
    "  # Create a dataset object and batch for the training data\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (tf.cast(train_images[..., tf.newaxis]/255, tf.float32),\n",
    "       tf.cast(train_labels, tf.int64)))\n",
    "  dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "  # Create a dataset object and batch for the test data\n",
    "  eval_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (tf.cast(test_images[..., tf.newaxis]/255, tf.float32),\n",
    "       tf.cast(test_labels, tf.int64)))\n",
    "  eval_dataset = eval_dataset.batch(10000)\n",
    "\n",
    "  # Define the model using tf.keras.layers\n",
    "  mnist_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(16, 8,\n",
    "                             strides=2,\n",
    "                             padding='same',\n",
    "                             activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(2, 1),\n",
    "      tf.keras.layers.Conv2D(32, 4, strides=2, activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(2, 1),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dd943ca98f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdpsgd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         grads_and_vars = opt.compute_gradients(loss_fn, var_list,\n\u001b[0;32m---> 67\u001b[0;31m                                                gradient_tape=gradient_tape)\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WORKDIR/dpsgd/tensorflow_privacy/privacy/tensorflow_privacy/privacy/optimizers/dp_optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss, gradient_tape)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_microbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m           \u001b[0msample_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_microbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         grad_sums, self._global_state = (\n",
      "\u001b[0;32m~/WORKDIR/dpsgd/tensorflow_privacy/privacy/tensorflow_privacy/privacy/optimizers/dp_optimizer.py\u001b[0m in \u001b[0;36mprocess_microbatch\u001b[0;34m(i, sample_state)\u001b[0m\n\u001b[1;32m    104\u001b[0m               input_tensor=tf.gather(microbatches_losses, [i]))\n\u001b[1;32m    105\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mgradient_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmicrobatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m           sample_state = self._dp_sum_query.accumulate_record(\n\u001b[1;32m    108\u001b[0m               sample_params, sample_state, grads)\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "  # Instantiate the optimizer\n",
    "  if dpsgd:\n",
    "    opt = DPGradientDescentGaussianOptimizer(\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        num_microbatches=microbatches,\n",
    "        learning_rate=learning_rate)\n",
    "  else:\n",
    "    opt = GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "  # Training loop.\n",
    "  steps_per_epoch = 60000 // batch_size\n",
    "  for epoch in range(epochs):\n",
    "    # Train the model for one epoch.\n",
    "    for (_, (images, labels)) in enumerate(dataset.take(-1)):\n",
    "      with tf.GradientTape(persistent=True) as gradient_tape:\n",
    "        # This dummy call is needed to obtain the var list.\n",
    "        logits = mnist_model(images, training=True)\n",
    "        var_list = mnist_model.trainable_variables\n",
    "\n",
    "        # In Eager mode, the optimizer takes a function that returns the loss.\n",
    "        def loss_fn():\n",
    "          logits = mnist_model(images, training=True)  # pylint: disable=undefined-loop-variable,cell-var-from-loop\n",
    "          loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "              labels=labels, logits=logits)  # pylint: disable=undefined-loop-variable,cell-var-from-loop\n",
    "          # If training without privacy, the loss is a scalar not a vector.\n",
    "          if not dpsgd:\n",
    "            loss = tf.reduce_mean(input_tensor=loss)\n",
    "          return loss\n",
    "\n",
    "        if dpsgd:\n",
    "          grads_and_vars = opt.compute_gradients(loss_fn, var_list,\n",
    "                                                 gradient_tape=gradient_tape)\n",
    "        else:\n",
    "          grads_and_vars = opt.compute_gradients(loss_fn, var_list)\n",
    "      print(grads_and_vars)\n",
    "      opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    for (_, (images, labels)) in enumerate(eval_dataset.take(-1)):\n",
    "      logits = mnist_model(images, training=False)\n",
    "      correct_preds = tf.equal(tf.argmax(input=logits, axis=1), labels)\n",
    "    test_accuracy = np.mean(correct_preds.numpy())\n",
    "    print('Test accuracy after epoch %d is: %.3f' % (epoch, test_accuracy))\n",
    "\n",
    "    # Compute the privacy budget expended so far.\n",
    "    if dpsgd:\n",
    "      eps = compute_epsilon((epoch + 1) * steps_per_epoch)\n",
    "      print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "    else:\n",
    "      print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "      with tf.GradientTape(persistent=True) as gradient_tape:\n",
    "        # This dummy call is needed to obtain the var list.\n",
    "        logits = mnist_model(images, training=True)\n",
    "        var_list = mnist_model.trainable_variables\n",
    "\n",
    "        # In Eager mode, the optimizer takes a function that returns the loss.\n",
    "        def loss_fn():\n",
    "          logits = mnist_model(images, training=True)  # pylint: disable=undefined-loop-variable,cell-var-from-loop\n",
    "          loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "              labels=labels, logits=logits)  # pylint: disable=undefined-loop-variable,cell-var-from-loop\n",
    "          # If training without privacy, the loss is a scalar not a vector.\n",
    "          if not dpsgd:\n",
    "            loss = tf.reduce_mean(input_tensor=loss)\n",
    "          return loss\n",
    "\n",
    "        if dpsgd:\n",
    "          grads_and_vars = opt.compute_gradients(loss_fn, var_list,\n",
    "                                                 gradient_tape=gradient_tape)\n",
    "        else:\n",
    "          grads_and_vars = opt.compute_gradients(loss_fn, var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 8, 1, 16), dtype=float32, numpy=\n",
       " array([[[[ 6.22760970e-03,  3.78497690e-03,  2.89283018e-03, ...,\n",
       "           -3.95402825e-03, -3.61379376e-03,  5.04491478e-03]],\n",
       " \n",
       "         [[-1.71382457e-03,  1.08591001e-03,  4.03887639e-03, ...,\n",
       "            7.12561305e-04,  3.86093417e-03, -1.03019807e-03]],\n",
       " \n",
       "         [[ 2.61681154e-03, -1.30665474e-04, -3.78490542e-03, ...,\n",
       "            6.87710941e-03,  6.90065930e-03,  3.73602874e-04]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.61003826e-04,  3.26182903e-03,  6.40055398e-04, ...,\n",
       "           -5.99179766e-04,  4.15064453e-04, -2.40638779e-04]],\n",
       " \n",
       "         [[-6.92460744e-04, -1.73666410e-03, -2.15681852e-03, ...,\n",
       "           -2.28105788e-03, -3.94116063e-03,  1.82628026e-03]],\n",
       " \n",
       "         [[-8.67268350e-03,  3.69376037e-03, -6.49256574e-04, ...,\n",
       "           -4.26838826e-03, -4.20047948e-03,  6.07611658e-03]]],\n",
       " \n",
       " \n",
       "        [[[-1.05375731e-02,  5.85369021e-03, -3.77016980e-03, ...,\n",
       "            2.36617750e-03, -6.99667493e-03,  5.35180792e-04]],\n",
       " \n",
       "         [[-5.78214601e-03, -1.76203647e-03,  6.08738139e-03, ...,\n",
       "           -1.47196371e-03,  9.98351607e-04,  1.59557778e-02]],\n",
       " \n",
       "         [[ 2.99128331e-03, -3.80299753e-03,  2.01064325e-03, ...,\n",
       "            6.31714053e-03,  6.44290308e-03,  5.50563261e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-2.06026970e-03, -6.74531003e-03, -5.06686419e-03, ...,\n",
       "            4.45815967e-03, -2.58739688e-03,  1.47447700e-03]],\n",
       " \n",
       "         [[ 5.20190410e-03,  4.03805729e-03, -1.14962342e-03, ...,\n",
       "            1.83065174e-04,  2.24823994e-03,  2.84832786e-03]],\n",
       " \n",
       "         [[ 6.71317102e-03,  3.15417862e-03, -1.83566147e-03, ...,\n",
       "           -7.27271242e-03,  8.31447728e-03, -3.87892104e-03]]],\n",
       " \n",
       " \n",
       "        [[[-6.92321220e-04, -5.65061346e-04,  7.47790048e-03, ...,\n",
       "            1.00608990e-02, -5.29047160e-04,  1.03281820e-02]],\n",
       " \n",
       "         [[ 3.96036683e-03,  2.62900395e-03, -7.97666784e-04, ...,\n",
       "            4.46528941e-03, -4.63073421e-03,  3.45836906e-03]],\n",
       " \n",
       "         [[-6.17244921e-04, -7.34694116e-03,  4.46946733e-03, ...,\n",
       "           -3.35209363e-04, -2.26730714e-03,  8.32870323e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.76349222e-03,  1.21681206e-03, -1.89578440e-03, ...,\n",
       "           -4.27772803e-03,  1.32900360e-03, -2.14430969e-03]],\n",
       " \n",
       "         [[ 1.30588247e-03,  3.68111697e-03, -4.93371347e-03, ...,\n",
       "           -3.16137302e-04,  8.46473093e-04, -4.20188159e-03]],\n",
       " \n",
       "         [[-1.74059439e-03,  6.03968091e-03, -1.91134342e-03, ...,\n",
       "            1.09627668e-03,  3.46767716e-03, -2.66724755e-03]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[-3.87553545e-03,  3.28220660e-03,  1.10113779e-02, ...,\n",
       "           -9.76364408e-03,  2.66396697e-03,  7.64655508e-03]],\n",
       " \n",
       "         [[ 3.35506722e-03,  7.85304524e-04, -3.86982760e-03, ...,\n",
       "           -5.93909482e-03, -2.52739992e-03,  5.23742894e-03]],\n",
       " \n",
       "         [[-9.64174746e-04,  1.54968258e-03, -2.63894442e-04, ...,\n",
       "           -1.37541462e-02, -8.28574691e-03, -2.63516908e-04]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-4.38485621e-03,  3.77636286e-03,  5.44130802e-04, ...,\n",
       "           -7.41327228e-03,  2.01509800e-03, -1.41266361e-03]],\n",
       " \n",
       "         [[-2.11538165e-03,  4.41691792e-03, -4.85174172e-03, ...,\n",
       "            2.57226464e-04,  1.11412944e-03, -2.96918117e-03]],\n",
       " \n",
       "         [[-3.34560918e-03, -6.62140828e-03, -2.77960999e-03, ...,\n",
       "            4.57348907e-03,  2.62090494e-03,  3.51879932e-03]]],\n",
       " \n",
       " \n",
       "        [[[ 4.74806642e-03,  4.27647540e-03, -1.22806069e-03, ...,\n",
       "           -5.62318787e-03,  2.42656461e-05,  2.93989200e-03]],\n",
       " \n",
       "         [[ 1.87191542e-03, -5.48568415e-03,  1.75067666e-03, ...,\n",
       "           -2.25947541e-03, -3.80952464e-04, -3.99103994e-03]],\n",
       " \n",
       "         [[-3.18562472e-03, -1.56696839e-03,  9.61669954e-04, ...,\n",
       "           -1.03972629e-02, -4.66406718e-03,  2.97016627e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 8.18025414e-03, -4.11092956e-03,  9.43387486e-03, ...,\n",
       "           -3.92358191e-03,  9.66530759e-03,  1.13138603e-02]],\n",
       " \n",
       "         [[ 6.78012241e-03, -2.93054210e-04, -2.86727818e-03, ...,\n",
       "           -5.63234696e-03, -3.43181170e-03, -8.92688986e-03]],\n",
       " \n",
       "         [[ 4.45863511e-03,  4.21821489e-04, -2.09809025e-03, ...,\n",
       "            1.67992548e-03,  6.24110410e-03, -7.18528638e-04]]],\n",
       " \n",
       " \n",
       "        [[[ 2.51265918e-03,  5.75947668e-03,  6.24706969e-03, ...,\n",
       "           -1.70069980e-03,  3.39355087e-03,  2.04362394e-03]],\n",
       " \n",
       "         [[ 2.32740049e-03,  1.29052605e-02, -3.16689559e-03, ...,\n",
       "           -3.96223785e-03,  4.61598579e-03, -2.38780398e-03]],\n",
       " \n",
       "         [[-8.41784570e-03, -3.00088688e-03,  2.77673360e-03, ...,\n",
       "           -4.47886472e-04, -2.13103858e-03,  7.62929535e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-2.08344683e-03,  6.21758681e-03,  1.21615885e-04, ...,\n",
       "            1.35771604e-03, -2.89226533e-04,  5.69070643e-03]],\n",
       " \n",
       "         [[ 7.19847716e-03,  2.88656563e-03,  8.58745631e-03, ...,\n",
       "           -7.11760670e-03,  5.35808795e-04, -2.45506014e-03]],\n",
       " \n",
       "         [[ 1.46390754e-03,  1.20598314e-04,  5.97517099e-03, ...,\n",
       "           -2.77313404e-03, -6.08889561e-04, -1.16206463e-02]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d/kernel:0' shape=(8, 8, 1, 16) dtype=float32, numpy=\n",
       " array([[[[ 0.06860888, -0.02651238, -0.03515136, ...,  0.03844657,\n",
       "           -0.0033575 ,  0.0474588 ]],\n",
       " \n",
       "         [[-0.01597131,  0.03090242,  0.03914634, ..., -0.04487906,\n",
       "           -0.05821234, -0.01901719]],\n",
       " \n",
       "         [[-0.03193798,  0.01231161,  0.00454194, ..., -0.05377048,\n",
       "           -0.0492283 ,  0.0226353 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0090916 ,  0.04229053,  0.06545804, ...,  0.06378944,\n",
       "            0.00660847, -0.04217685]],\n",
       " \n",
       "         [[-0.06770991, -0.03562924,  0.04321478, ..., -0.03383258,\n",
       "           -0.05436166, -0.03411858]],\n",
       " \n",
       "         [[-0.02434707, -0.01357459,  0.06904753, ..., -0.02585585,\n",
       "           -0.01378982,  0.03038397]]],\n",
       " \n",
       " \n",
       "        [[[-0.02010595, -0.06310303,  0.04716327, ..., -0.02366493,\n",
       "            0.0145006 ,  0.06326408]],\n",
       " \n",
       "         [[-0.04855627,  0.04628333,  0.06528488, ..., -0.01580926,\n",
       "            0.01924307, -0.01221575]],\n",
       " \n",
       "         [[-0.05943716, -0.03070571, -0.02965437, ...,  0.05807836,\n",
       "            0.07116402, -0.05159053]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.02726701, -0.03223547, -0.01880049, ...,  0.06978931,\n",
       "           -0.04109336,  0.03792962]],\n",
       " \n",
       "         [[-0.00222217,  0.06894169,  0.0095051 , ...,  0.00489291,\n",
       "           -0.05580353,  0.02134323]],\n",
       " \n",
       "         [[-0.00930407,  0.06842893, -0.05337407, ..., -0.04962317,\n",
       "           -0.03108694,  0.0079653 ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.05655166,  0.05026598, -0.01977571, ...,  0.070801  ,\n",
       "            0.00381531,  0.05859787]],\n",
       " \n",
       "         [[-0.04371252, -0.01656403,  0.04226425, ..., -0.0394321 ,\n",
       "           -0.04215712,  0.06381154]],\n",
       " \n",
       "         [[-0.02595391,  0.02171639,  0.0381641 , ..., -0.06343074,\n",
       "            0.06432909,  0.0510025 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.01961969,  0.04688384,  0.04542346, ..., -0.058271  ,\n",
       "            0.05738556,  0.07165126]],\n",
       " \n",
       "         [[ 0.04731793,  0.04407927,  0.02054385, ...,  0.00536872,\n",
       "            0.05054127, -0.07068686]],\n",
       " \n",
       "         [[-0.05128316, -0.01766299, -0.03184136, ..., -0.02527519,\n",
       "           -0.03216336, -0.00245134]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[-0.03754515,  0.02216636,  0.00891303, ..., -0.03140572,\n",
       "           -0.07453972, -0.00125507]],\n",
       " \n",
       "         [[ 0.03914915, -0.03380778,  0.06125291, ..., -0.01881085,\n",
       "           -0.03481661, -0.06460825]],\n",
       " \n",
       "         [[-0.03103505,  0.03856093,  0.0540406 , ..., -0.02958514,\n",
       "           -0.01256338, -0.04767193]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.04836141,  0.02476238, -0.06652774, ...,  0.0495307 ,\n",
       "            0.02490316,  0.02117679]],\n",
       " \n",
       "         [[ 0.01087695,  0.0437219 , -0.01108748, ...,  0.01900718,\n",
       "            0.04583114, -0.05392785]],\n",
       " \n",
       "         [[ 0.04201585, -0.03134068, -0.06896959, ...,  0.00484261,\n",
       "            0.03143609, -0.00151209]]],\n",
       " \n",
       " \n",
       "        [[[-0.01312508, -0.00312653,  0.05308275, ...,  0.02359682,\n",
       "            0.01572511, -0.00578176]],\n",
       " \n",
       "         [[ 0.01444012,  0.06204391,  0.0640734 , ..., -0.04502073,\n",
       "           -0.03759657,  0.04865914]],\n",
       " \n",
       "         [[ 0.03160277, -0.04994403,  0.0372848 , ...,  0.06529416,\n",
       "           -0.02297511, -0.00684355]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.01274648, -0.06895865, -0.02503478, ...,  0.05963031,\n",
       "            0.02613016,  0.05355129]],\n",
       " \n",
       "         [[ 0.06289189,  0.04475888,  0.01461517, ...,  0.01848889,\n",
       "            0.07183283, -0.00856172]],\n",
       " \n",
       "         [[ 0.06633598,  0.05536303,  0.01807142, ...,  0.07151592,\n",
       "            0.00525888,  0.03636798]]],\n",
       " \n",
       " \n",
       "        [[[-0.05324347, -0.02345228,  0.02317745, ...,  0.00412109,\n",
       "            0.06003809,  0.04676725]],\n",
       " \n",
       "         [[-0.04861233, -0.01672645,  0.06571684, ..., -0.01922302,\n",
       "           -0.04536135, -0.02620819]],\n",
       " \n",
       "         [[-0.01691488,  0.02329641, -0.00543985, ..., -0.04757429,\n",
       "            0.00855519, -0.06483351]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.05840975,  0.06924968, -0.06213032, ...,  0.0210497 ,\n",
       "           -0.01674736, -0.04389723]],\n",
       " \n",
       "         [[ 0.04384314, -0.03402666, -0.01547852, ..., -0.06006039,\n",
       "            0.02810274,  0.05121628]],\n",
       " \n",
       "         [[ 0.01649914, -0.07142913,  0.00564406, ...,  0.00893976,\n",
       "            0.038976  , -0.00272661]]]], dtype=float32)>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_and_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
