{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic functions has been put in to dpsgd_keras.py, won't touch this notebook, for it already works.\n",
    "\n",
    "# Copyright 2019, The TensorFlow Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Training a CNN on MNIST with Keras and the DP SGD optimizer.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(3)\n",
    "dpsgd = True\n",
    "learning_rate = 0.015\n",
    "noise_multiplier = 5\n",
    "l2_norm_clip = 1.0\n",
    "batch_size = 25\n",
    "epochs = 10\n",
    "microbatches = 25\n",
    "num_parameters = 50\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice_cond(x, size):\n",
    "    tensor_size = tf.size(x)\n",
    "    indices = tf.range(0, tensor_size, dtype=tf.int64)\n",
    "    if size == 0:\n",
    "        sample_flatten_index = tf.random.shuffle(indices)[:]\n",
    "    else:     \n",
    "        sample_flatten_index = tf.random.shuffle(indices)[:size]\n",
    "    sample_index = tf.transpose(tf.unravel_index(tf.cast(sample_flatten_index,tf.int32), tf.shape(input=x))) #[[all 0-th dimension indexes], [all 1-th dimension indexes]]\n",
    "    cond = tf.scatter_nd(sample_index, tf.ones(tf.shape(input=sample_index)[0],dtype=tf.bool), tf.shape(input=x))\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need noise on the same fixed number of model parameters for each layer.\n",
    "\n",
    "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "def make_fixed_keras_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP Keras optimizer class from an existing one.\"\"\"\n",
    "\n",
    "  class FixedDPOptimizerClass(cls):\n",
    "    \"\"\"Differentially private subclass of given class cls.\n",
    "    The class tf.keras.optimizers.Optimizer has two methods to compute\n",
    "    gradients, `_compute_gradients` and `get_gradients`. The first works\n",
    "    with eager execution, while the second runs in graph mode and is used\n",
    "    by canned estimators.\n",
    "    Internally, DPOptimizerClass stores hyperparameters both individually\n",
    "    and encapsulated in a `GaussianSumQuery` object for these two use cases.\n",
    "    However, this should be invisible to users of this class.\n",
    "    \n",
    "    \n",
    "    btw. support negative num_parameters, used for all but num_parameters noises.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2_norm_clip,\n",
    "        noise_multiplier,\n",
    "        num_microbatches=None,\n",
    "        num_parameters = 10,\n",
    "        #noise_layer_type = \"linear\",\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
    "        **kwargs):\n",
    "      \"\"\"Initialize the DPOptimizerClass.\n",
    "      Args:\n",
    "        l2_norm_clip: Clipping norm (max L2 norm of per microbatch gradients)\n",
    "        noise_multiplier: Ratio of the standard deviation to the clipping norm\n",
    "        num_microbatches: The number of microbatches into which each minibatch\n",
    "          is split.\n",
    "      \"\"\"\n",
    "      super(FixedDPOptimizerClass, self).__init__(*args, **kwargs)\n",
    "      self._l2_norm_clip = l2_norm_clip\n",
    "      self._noise_multiplier = noise_multiplier\n",
    "      self._num_microbatches = num_microbatches\n",
    "      self._dp_sum_query = gaussian_query.GaussianSumQuery(\n",
    "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
    "      self._global_state = None\n",
    "      self._num_parameters = num_parameters\n",
    "      self._was_dp_gradients_called = False\n",
    "      self.samples_cond = {}\n",
    "\n",
    "    def _compute_gradients(self, loss, var_list, grad_loss=None, tape=None):\n",
    "      \"\"\"DP version of superclass method.\"\"\"\n",
    "      print(\"compute the gradient\")\n",
    "      is_considered = [x.name.startswith(\"Considered\") for x in var_list]\n",
    "    \n",
    "      print(is_considered)\n",
    "      self._was_dp_gradients_called = True\n",
    "      # Precompute the noise locations\n",
    "      if len(self.samples_cond) == 0:\n",
    "        self.samples_cond = [tf.Variable(random_choice_cond(x, self._num_parameters)) for x in var_list]\n",
    "      # Compute loss.\n",
    "      if not callable(loss) and tape is None:\n",
    "        raise ValueError('`tape` is required when a `Tensor` loss is passed.')\n",
    "      tape = tape if tape is not None else tf.GradientTape()\n",
    "      self.is_considered = is_considered\n",
    "      if callable(loss):\n",
    "        with tape:\n",
    "          if not callable(var_list):\n",
    "            tape.watch(var_list)\n",
    "\n",
    "          if callable(loss):\n",
    "            loss = loss()\n",
    "            microbatch_losses = tf.reduce_mean(\n",
    "                tf.reshape(loss, [self._num_microbatches, -1]), axis=1)\n",
    "\n",
    "          if callable(var_list):\n",
    "            var_list = var_list()\n",
    "      else:\n",
    "        with tape:\n",
    "          microbatch_losses = tf.reduce_mean(\n",
    "              tf.reshape(loss, [self._num_microbatches, -1]), axis=1)\n",
    "\n",
    "      var_list = tf.nest.flatten(var_list)\n",
    "\n",
    "      # Compute the per-microbatch losses using helpful jacobian method.\n",
    "      with tf.keras.backend.name_scope(self._name + '/gradients'):\n",
    "        jacobian = tape.jacobian(microbatch_losses, var_list)\n",
    "\n",
    "        # Clip gradients to given l2_norm_clip.\n",
    "        def clip_gradients(g):\n",
    "          return tf.clip_by_global_norm(g, self._l2_norm_clip)[0]\n",
    "\n",
    "        clipped_gradients = tf.map_fn(clip_gradients, jacobian)\n",
    "        \n",
    "        def reduce_noise_normalize_batch(self, g, is_considered, layer_index):\n",
    "          # Sum gradients over all microbatches.\n",
    "          summed_gradient = tf.reduce_sum(g, axis=0)\n",
    "\n",
    "          # Sample the indexes\n",
    "          sampled_cond = self.samples_cond[layer_index]\n",
    "          '''\n",
    "          is_linear = tf.rank(input=g) > 2\n",
    "          if self.noise_layer_type == 'linear':\n",
    "            sampled_cond = tf.math.logical_or(sampled_cond, tf.math.logical_not(is_linear))\n",
    "            #sampled_cond = tf.math.logical_and(sampled_cond, is_linear)\n",
    "          elif self.noise_layer_type == 'bias':\n",
    "            sampled_cond = tf.math.logical_or(sampled_cond, is_linear)\n",
    "            #sampled_cond = tf.math.logical_and(sampled_cond, tf.math.logical_not(is_linear))\n",
    "          else:\n",
    "            assert(False)\n",
    "          '''\n",
    "          # Add noise to summed gradients.\n",
    "          noise_stddev = self._l2_norm_clip * self._noise_multiplier\n",
    "          noise = tf.random.normal(\n",
    "              tf.shape(input=summed_gradient), stddev=noise_stddev)\n",
    "          noised_gradient = tf.add(summed_gradient, noise)\n",
    "          #tf.print(layer_index)\n",
    "          #if layer_index == 2:\n",
    "          #    tf.print(sampled_cond)\n",
    "          #tf.print(\"num of noise:\", tf.math.reduce_sum(tf.cast(tf.math.logical_or(sampled_cond, tf.math.logical_not(is_considered)), tf.int32)))\n",
    "          fixed_gradient = tf.where(tf.math.logical_and(sampled_cond, is_considered), noised_gradient, summed_gradient)\n",
    "          # Normalize by number of microbatches and return.\n",
    "          return tf.truediv(fixed_gradient, self._num_microbatches)\n",
    "            \n",
    "\n",
    "        final_gradients = [reduce_noise_normalize_batch(self, clipped_gradients[i], is_considered[i], i) for i in range(len(clipped_gradients))]\n",
    "        \n",
    "        \n",
    "        self.clipped_gradients = clipped_gradients\n",
    "        self.final_gradients = final_gradients\n",
    "      return list(zip(final_gradients, var_list))\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "      assert self._was_dp_gradients_called, (\n",
    "          'Neither _compute_gradients() or get_gradients() on the '\n",
    "          'differentially private optimizer was called. This means the '\n",
    "          'training is not differentially private. It may be the case that '\n",
    "          'you need to upgrade to TF 2.4 or higher to use this particular '\n",
    "          'optimizer.')\n",
    "      return super(FixedDPOptimizerClass,\n",
    "                   self).apply_gradients(grads_and_vars, global_step, name)\n",
    "\n",
    "  return FixedDPOptimizerClass\n",
    "\n",
    "FixedDPKerasSGDOptimizer = make_fixed_keras_optimizer_class(tf.keras.optimizers.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epsilon(steps, sampling_probability):\n",
    "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "  if noise_multiplier == 0.0:\n",
    "    return float('inf')\n",
    "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "  rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=noise_multiplier,\n",
    "                    steps=steps,\n",
    "                    orders=orders)\n",
    "  # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n",
    "  test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(BiasLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight('bias',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "    def call(self, x):\n",
    "        return x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = tf.keras.Sequential([\\n    tf.keras.layers.Conv2D(16, 8,\\n                         strides=2,\\n                         padding='same',\\n                         activation='relu',\\n                         input_shape=(28, 28, 1),\\n                          trainable = False),\\n    tf.keras.layers.MaxPool2D(2, 1),\\n    tf.keras.layers.Conv2D(32, 4,\\n                         strides=2,\\n                         padding='valid',\\n                         activation='relu'),\\n    tf.keras.layers.MaxPool2D(2, 1),\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dense(32, activation='relu'),\\n    tf.keras.layers.Dense(10)\\n])\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def main_simple(unused_argv):\n",
    "logging.set_verbosity(logging.INFO)\n",
    "if dpsgd and batch_size % microbatches != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "# Define a sequential Keras model\n",
    "'''\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 8,\n",
    "                         strides=2,\n",
    "                         padding='same',\n",
    "                         activation='relu',\n",
    "                         input_shape=(28, 28, 1),\n",
    "                          trainable = False),\n",
    "    tf.keras.layers.MaxPool2D(2, 1),\n",
    "    tf.keras.layers.Conv2D(32, 4,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2, 1),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "def build_models(noise_layer_name):\n",
    "    if noise_layer_name==\"conv_linear1\":\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(16, 8,\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered\"),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Conv2D(32, 4,\n",
    "                                 strides=2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu', use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(32, activation='relu', use_bias=False),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(10, use_bias=False)\n",
    "        ])\n",
    "    elif noise_layer_name==\"conv_linear2\":\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(16, 8,\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Conv2D(32, 4,\n",
    "                                 strides=2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu', use_bias=False, name=\"Considered\"),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(32, activation='relu', use_bias=False),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(10, use_bias=False)\n",
    "        ]) \n",
    "    elif noise_layer_name==\"conv_bias1\":\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(16, 8,\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          BiasLayer(name=\"Considered1\"),\n",
    "          tf.keras.layers.Conv2D(32, 4,\n",
    "                                 strides=2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu', use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(32, activation='relu', use_bias=False),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(10, use_bias=False)\n",
    "        ])\n",
    "    elif noise_layer_name==\"conv_bias2\":\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(16, 8,\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Conv2D(32, 4,\n",
    "                                 strides=2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu', use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          BiasLayer(name=\"Considered2\"),\n",
    "          tf.keras.layers.Dense(32, activation='relu', use_bias=False),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(10, use_bias=False)\n",
    "        ])\n",
    "    elif noise_layer_name==\"fully_linear\":\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(16, 8,\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Conv2D(32, 4,\n",
    "                                 strides=2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu', use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(32, activation='relu', use_bias=False, name=\"Considered\"),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(10, use_bias=False)\n",
    "        ])\n",
    "    elif noise_layer_name==\"fully_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(16, 8,\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Conv2D(32, 4,\n",
    "                                 strides=2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu', use_bias=False),\n",
    "          tf.keras.layers.MaxPool2D(2, 1),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          BiasLayer(),\n",
    "          tf.keras.layers.Dense(32, activation='relu', use_bias=False),\n",
    "          BiasLayer(name=\"Considered\"),\n",
    "          tf.keras.layers.Dense(10, use_bias=False)\n",
    "        ])\n",
    "    else:\n",
    "        assert(False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 16)        1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "bias_layer (BiasLayer)       (None, 13, 13, 16)        2704      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 5, 32)          8192      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "bias_layer_1 (BiasLayer)     (None, 512)               512       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                16384     \n",
      "_________________________________________________________________\n",
      "Considered (BiasLayer)       (None, 32)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                320       \n",
      "=================================================================\n",
      "Total params: 29,168\n",
      "Trainable params: 29,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"fully_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "compute the gradient\n",
      "[True, False, False, False, False, False, False]\n",
      "compute the gradient\n",
      "[True, False, False, False, False, False, False]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_1/Considered/Conv2D (defined at <ipython-input-11-26faeb4ec1f5>:72) ]] [Op:__inference_train_function_3163]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-26faeb4ec1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     callbacks = [model_checkpoint_callback, early_stopping_callback], workers=1)\n\u001b[0m\u001b[1;32m     73\u001b[0m         evaluated_result = model.evaluate(\n\u001b[1;32m     74\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_1/Considered/Conv2D (defined at <ipython-input-11-26faeb4ec1f5>:72) ]] [Op:__inference_train_function_3163]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#For bias first experiment.\n",
    "model = tf.keras.Sequential([\n",
    "  BiasLayer(),\n",
    "  tf.keras.layers.Conv2D(16, 8,\n",
    "                         strides=2,\n",
    "                         padding='same',\n",
    "                         activation='relu',\n",
    "                         input_shape=(28, 28, 1), use_bias=False),\n",
    "  tf.keras.layers.MaxPool2D(2, 1),\n",
    "  BiasLayer(),\n",
    "  tf.keras.layers.Conv2D(32, 4,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         activation='relu', use_bias=False),\n",
    "  tf.keras.layers.MaxPool2D(2, 1),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  BiasLayer(),\n",
    "  tf.keras.layers.Dense(32, activation='relu', use_bias=False),\n",
    "  BiasLayer(),\n",
    "  tf.keras.layers.Dense(10, use_bias=False)\n",
    "])\n",
    "'''\n",
    "\n",
    "parameter_list = {}\n",
    "accuracies = {}\n",
    "for num_parameters in [1,500,1000]:\n",
    "    for noise_layer_type in [\"conv_linear1\",\"conv_linear2\", \"conv_bias1\",\"conv_bias2\", \"fully_linear\", \"fully_bias\"]:\n",
    "        file_name = str(num_parameters) + \"_\" + noise_layer_type\n",
    "        model = build_models(noise_layer_type)\n",
    "        '''\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            BiasLayer(name='bias1'),\n",
    "            tf.keras.layers.Dense(32, use_bias=False, activation=None, name='linear1'),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            BiasLayer(name='Consideredbias2'),\n",
    "            tf.keras.layers.Dense(10, use_bias=False, name='Consideredlinear2'),\n",
    "        ])'''\n",
    "        if dpsgd:\n",
    "            optimizer = FixedDPKerasSGDOptimizer(\n",
    "                num_parameters = num_parameters,\n",
    "                l2_norm_clip=l2_norm_clip,\n",
    "                noise_multiplier=noise_multiplier,\n",
    "                num_microbatches=microbatches,\n",
    "                learning_rate=learning_rate)\n",
    "            # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "            loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "                from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "            loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # Compile model with Keras\n",
    "        checkpoint_filepath = \"./data/\"+file_name\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "          filepath=checkpoint_filepath,\n",
    "          save_weights_only=True,\n",
    "          monitor='val_accuracy',\n",
    "          mode='max',\n",
    "          save_best_only=True)\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', min_delta=0, patience=1, verbose=0,\n",
    "            mode='auto', baseline=None, restore_best_weights=True\n",
    "        )\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        # Train model with Keras\n",
    "        model.fit(train_data, train_labels,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(test_data, test_labels),\n",
    "                    batch_size=batch_size, \n",
    "                    callbacks = [model_checkpoint_callback, early_stopping_callback], workers=1)\n",
    "        evaluated_result = model.evaluate(\n",
    "            x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "            callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "            return_dict=True)\n",
    "        if noise_layer_type not in accuracies.keys():\n",
    "            accuracies[noise_layer_type] = []\n",
    "        if noise_layer_type not in parameter_list.keys():\n",
    "            parameter_list[noise_layer_type] = []\n",
    "        accuracies[noise_layer_type].append(evaluated_result[\"accuracy\"])\n",
    "        parameter_list[noise_layer_type].append(num_parameters)\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * len(train_data) // batch_size, len(train_data)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "for key in parameter_list.keys():\n",
    "    line1, = ax.plot(parameter_list[key],accuracies[key],  label=key)\n",
    "    line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee\n",
    "import pickle\n",
    "result = open(\"result\",\"wb\")\n",
    "pickle.dump(accuracies, result)\n",
    "pickle.dump(parameter_list, result)\n",
    "result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(\"result\", \"rb\")\n",
    "accuracies = pickle.load(result)\n",
    "parameter_list = pickle.load(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "line1, = ax.plot(parameter_list[\"linear\"],accuracies[\"linear\"],  label='Linear')\n",
    "line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "line2, = ax.plot(parameter_list[\"bias\"],  accuracies[\"bias\"], label='Bias')\n",
    "line2.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
