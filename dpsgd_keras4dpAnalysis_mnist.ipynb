{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: 2021/03/14\n",
    "# Usage: Dp analysis of mnist neural network.\n",
    "# Detail: Check the notion notes in HE-DP project.\n",
    "\n",
    "from dpsgd_keras_slow import *\n",
    "\n",
    "dpsgd = False # add dp noise or not \n",
    "learning_rate = 0.1\n",
    "noise_multiplier = 2\n",
    "l2_norm_clip = 3\n",
    "batch_size = 512\n",
    "epochs = 120\n",
    "microbatch_size = 64\n",
    "num_parameters = 0\n",
    "privacy_budget = []\n",
    "delta = 1e-5  # it is recommended to use delta~=1/dataset_size\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n",
    "  test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturnbing the input dataset, and collect the accuracy-step\n",
    "class GaussianNoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, l2_norm_clip, noise_multiplier, *args, **kwargs):\n",
    "        super(GaussianNoiseLayer, self).__init__(*args, **kwargs)\n",
    "        self._l2_norm_clip = l2_norm_clip\n",
    "        self._noise_multiplier = noise_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Clip gradients to given l2_norm_clip.\n",
    "        def clip_features(x):\n",
    "            return tf.clip_by_global_norm([x], self._l2_norm_clip)[0][0]\n",
    "\n",
    "        clipped_features = tf.map_fn(clip_features, x)\n",
    "        \n",
    "        # Add noise to summed gradients.\n",
    "        noise_stddev = self._l2_norm_clip * self._noise_multiplier\n",
    "        noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "        return clipped_features + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.clip_by_global_norm([tf.Variable([[2,3],[4,5]],shape=[2,2],dtype=tf.float32)], 4.4)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "def build_models(noise_layer_name):\n",
    "    if noise_layer_name ==\"mnist+untied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])        \n",
    "    elif noise_layer_name == \"mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])  \n",
    "    elif noise_layer_name == \"sphinx+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"sphinx+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(name=\"Considered2\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            BiasLayer(name=\"Considered8\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(name=\"Considered7\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(name=\"Considered8\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10\":\n",
    "        pass\n",
    "    else:\n",
    "        model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_layer (Gaussi (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "bias_layer (BiasLayer)       (None, 24, 24, 16)        9216      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "bias_layer_1 (BiasLayer)     (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_2 (BiasLayer)     (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_3 (BiasLayer)     (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 43,750\n",
      "Trainable params: 43,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist+untied_bias+noise_input\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "bias_layer_4 (BiasLayer)     (None, 24, 24, 16)        9216      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "bias_layer_5 (BiasLayer)     (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_6 (BiasLayer)     (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_7 (BiasLayer)     (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 43,750\n",
      "Trainable params: 43,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist+untied_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "tied_bias_layer (TiedBiasLay (None, 24, 24, 16)        16        \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_1 (TiedBiasL (None, 8, 8, 16)          16        \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_8 (BiasLayer)     (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_9 (BiasLayer)     (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 33,542\n",
      "Trainable params: 33,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist+tied_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_10 (BiasLayer)    (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_11 (BiasLayer)    (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 33,510\n",
      "Trainable params: 33,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main_simple(unused_argv):\n",
    "logging.set_verbosity(logging.INFO)\n",
    "if dpsgd and batch_size % microbatch_size != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // microbatch_size // 1000,\n",
    "    decay_rate=0.998)\n",
    "\n",
    "\n",
    "lr_schedule_no_dp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // batch_size // 1000,\n",
    "    decay_rate=0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "938/938 [==============================] - 143s 118ms/step - loss: 1.7240 - accuracy: 0.4656 - val_loss: 0.5683 - val_accuracy: 0.8158\n",
      "Epoch 2/120\n",
      "938/938 [==============================] - 67s 72ms/step - loss: 0.5502 - accuracy: 0.8319 - val_loss: 0.4759 - val_accuracy: 0.8709\n",
      "Epoch 3/120\n",
      "938/938 [==============================] - 67s 71ms/step - loss: 0.4792 - accuracy: 0.8694 - val_loss: 0.4668 - val_accuracy: 0.8816\n",
      "Epoch 4/120\n",
      "938/938 [==============================] - 66s 70ms/step - loss: 0.4824 - accuracy: 0.8808 - val_loss: 0.4389 - val_accuracy: 0.8925\n",
      "Epoch 5/120\n",
      "155/938 [===>..........................] - ETA: 54s - loss: 0.4581 - accuracy: 0.8906"
     ]
    }
   ],
   "source": [
    "# check the training accuracy\n",
    "dpsgd = True\n",
    "parameter_list = {}\n",
    "accuracies = {}\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\", \"ALLnoise+mnist+tied_bias\",\"mnist+tied_bias\", \"sphinx+mnist+untied_bias\", \"ALLnoise+mnist+untied_bias\",\"mnist+untied_bias\", \"mnist+untied_bias+noise_input\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+untied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=lr_schedule)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule_no_dp)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_noise(x, clip, noise_multiplier):\n",
    "    def clip_features(x):\n",
    "        return tf.clip_by_global_norm([x], clip)[0][0]\n",
    "    clipped_features = tf.map_fn(clip_features, x)\n",
    "    # Add noise to summed gradients.\n",
    "    noise_stddev = clip * noise_multiplier\n",
    "    noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "    return clipped_features + noise\n",
    "\n",
    "noisy_train_data = tf.map_fn(lambda x: clip_noise(x, l2_norm_clip, noise_multiplier), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training accuracy\n",
    "\n",
    "parameter_list = {}\n",
    "model_type = \"noisy_permanent_input\"\n",
    "dpsgd = False\n",
    "file_name = model_type + \"_accuracy\"\n",
    "model = build_models(\"mnist+untied_bias\")\n",
    "if dpsgd:\n",
    "    optimizer = FixedDPKerasSGDOptimizer(\n",
    "        batch_size = batch_size,\n",
    "        num_parameters = num_parameters,\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        var_list = model.trainable_variables,\n",
    "        #num_microbatches=batch_size//microbatch_size,\n",
    "        microbatch_size = microbatch_size,\n",
    "        learning_rate=learning_rate)\n",
    "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with Keras\n",
    "checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=checkpoint_filepath,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_accuracy',\n",
    "  mode='max',\n",
    "  save_best_only=True)\n",
    "#early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "#    mode='auto', baseline=None, restore_best_weights=True\n",
    "#)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "# Train model with Keras\n",
    "if dpsgd:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=microbatch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "else:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=batch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "\n",
    "#evaluated_result = model.evaluate(\n",
    "#    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "#    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "#    return_dict=True)\n",
    "if model_type not in accuracies.keys():\n",
    "    accuracies[model_type] = []\n",
    "accuracies[model_type].append(history)\n",
    "#accuracies[model_type].append(evaluated_result[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies = dict()\n",
    "for key in accuracies.keys():   \n",
    "    processed_accuracies[key] = accuracies[key][0].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['val_accuracy'])\n",
    "#plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"dp_data/results/accuracies_%.1f_%.1f_%.1f_%d\"%(learning_rate, noise_multiplier, l2_norm_clip, batch_size),\"wb\")\n",
    "pickle.dump(processed_accuracies, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the step-epsilon curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "try:\n",
    "    with open(\"dp_data/noise_epsilon_step\") as f:\n",
    "        epsilons = pickle.load(f)\n",
    "except:\n",
    "    training_data_size = 60000\n",
    "    max_step = 25000\n",
    "    epsilons = [[] for i in range(10)]\n",
    "    for noise_multiplier in range(0, 10, 1):\n",
    "        print(\"noise_multiplier:\", noise_multiplier)\n",
    "        for step in range(1, max_step, 10):\n",
    "            epsilons[noise_multiplier].append(compute_epsilon(step, batch_size/training_data_size, noise_multiplier))\n",
    "    file = open(\"dp_data/noise_epsilon_step_%d\"%batch_size,\"wb\")\n",
    "    pickle.dump(epsilons, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tied Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tied bias\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\", \"ALLnoise+mnist+tied_bias\",\"mnist+tied_bias\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+tied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=learning_rate)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "for key in parameter_list.keys():\n",
    "    line1, = ax.plot(parameter_list[key],accuracies[key],  label=key)\n",
    "    line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee\n",
    "import pickle\n",
    "result = open(\"result\",\"wb\")\n",
    "pickle.dump(accuracies, result)\n",
    "pickle.dump(parameter_list, result)\n",
    "result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(\"result\", \"rb\")\n",
    "accuracies = pickle.load(result)\n",
    "parameter_list = pickle.load(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "line1, = ax.plot(parameter_list[\"linear\"],accuracies[\"linear\"],  label='Linear')\n",
    "line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "line2, = ax.plot(parameter_list[\"bias\"],  accuracies[\"bias\"], label='Bias')\n",
    "line2.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "a = [tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32), tf.constant([3., 4, 5], shape=(3,), dtype=tf.float32), tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32)]\n",
    "b = tf.ones_like(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linalg.global_norm(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_global_norm(a, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
