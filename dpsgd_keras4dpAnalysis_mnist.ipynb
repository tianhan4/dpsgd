{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 17:42:26.995425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Experiment: 2021/03/14\n",
    "# Usage: Dp analysis of mnist neural network.\n",
    "# Detail: Check the notion notes in HE-DP project.\n",
    "\n",
    "from dpsgd_keras_slow import *\n",
    "from dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "\n",
    "dpsgd = True # add dp noise or not \n",
    "learning_rate = 0.1\n",
    "noise_multiplier = 4\n",
    "l2_norm_clip = 3\n",
    "batch_size = 10#512\n",
    "epochs = 1\n",
    "microbatch_size = 1#64\n",
    "num_parameters = 0\n",
    "privacy_budget = []\n",
    "delta = 1e-5  # it is recommended to use delta~=1/dataset_size\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n",
    "  test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturnbing the input dataset, and collect the accuracy-step\n",
    "class GaussianNoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, l2_norm_clip, noise_multiplier, *args, **kwargs):\n",
    "        super(GaussianNoiseLayer, self).__init__(*args, **kwargs)\n",
    "        self._l2_norm_clip = l2_norm_clip\n",
    "        self._noise_multiplier = noise_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Clip gradients to given l2_norm_clip.\n",
    "        def clip_features(x):\n",
    "            return tf.clip_by_global_norm([x], self._l2_norm_clip)[0][0]\n",
    "\n",
    "        clipped_features = tf.map_fn(clip_features, x)\n",
    "        \n",
    "        # Add noise to summed gradients.\n",
    "        noise_stddev = self._l2_norm_clip * self._noise_multiplier\n",
    "        noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "        return clipped_features + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.clip_by_global_norm([tf.Variable([[2,3],[4,5]],shape=[2,2],dtype=tf.float32)], 4.4)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "def build_models(noise_layer_name):\n",
    "    if noise_layer_name ==\"mnist+untied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])        \n",
    "    elif noise_layer_name == \"mnist+tied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])   \n",
    "    elif noise_layer_name == \"mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])  \n",
    "    elif noise_layer_name == \"sphinx+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"sphinx+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(name=\"Considered2\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            BiasLayer(name=\"Considered8\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(name=\"Considered7\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(name=\"Considered8\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10\":\n",
    "        pass\n",
    "    else:\n",
    "        model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Considered1 (Conv2D)        (None, 24, 24, 16)        400       \n",
      "                                                                 \n",
      " bias_layer (BiasLayer)      (None, 24, 24, 16)        9216      \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 24, 24, 16)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " Considered2 (Conv2D)        (None, 8, 8, 16)          6400      \n",
      "                                                                 \n",
      " bias_layer_1 (BiasLayer)    (None, 8, 8, 16)          1024      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 8, 8, 16)          0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " Considered3 (Dense)         (None, 100)               25600     \n",
      "                                                                 \n",
      " bias_layer_2 (BiasLayer)    (None, 100)               100       \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 100)               0         \n",
      "                                                                 \n",
      " Considered4 (Dense)         (None, 10)                1000      \n",
      "                                                                 \n",
      " bias_layer_3 (BiasLayer)    (None, 10)                10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,750\n",
      "Trainable params: 43,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"sphinx+mnist+untied_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main_simple(unused_argv):\n",
    "logging.set_verbosity(logging.INFO)\n",
    "if dpsgd and batch_size % microbatch_size != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // microbatch_size // 1000,\n",
    "    decay_rate=0.998)\n",
    "\n",
    "\n",
    "lr_schedule_no_dp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // batch_size // 1000,\n",
    "    decay_rate=0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, True, False, True, False, True, False]\n",
      "[True, False, True, False, True, False, True, False]\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "   1/6000 [..............................] - ETA: 6:14:14 - loss: 2.2665 - accuracy: 0.0000e+00jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "   4/6000 [..............................] - ETA: 1:55 - loss: 2.2846 - accuracy: 0.1000       jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "   7/6000 [..............................] - ETA: 1:52 - loss: 2.2831 - accuracy: 0.1429jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  10/6000 [..............................] - ETA: 1:51 - loss: 2.3079 - accuracy: 0.1200jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  13/6000 [..............................] - ETA: 1:58 - loss: 2.5728 - accuracy: 0.1154jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  16/6000 [..............................] - ETA: 2:00 - loss: 2.8080 - accuracy: 0.1250jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  19/6000 [..............................] - ETA: 2:04 - loss: 2.9593 - accuracy: 0.1211jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  22/6000 [..............................] - ETA: 2:06 - loss: 3.2475 - accuracy: 0.1318jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  25/6000 [..............................] - ETA: 2:07 - loss: 3.8793 - accuracy: 0.1280jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  28/6000 [..............................] - ETA: 2:07 - loss: 4.0862 - accuracy: 0.1357jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  31/6000 [..............................] - ETA: 2:07 - loss: 4.3318 - accuracy: 0.1258jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  33/6000 [..............................] - ETA: 2:11 - loss: 4.4443 - accuracy: 0.1333jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  35/6000 [..............................] - ETA: 2:15 - loss: 4.7047 - accuracy: 0.1343jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  38/6000 [..............................] - ETA: 2:14 - loss: 4.7691 - accuracy: 0.1316jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  41/6000 [..............................] - ETA: 2:13 - loss: 4.9214 - accuracy: 0.1220jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  44/6000 [..............................] - ETA: 2:13 - loss: 4.9713 - accuracy: 0.1250jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  47/6000 [..............................] - ETA: 2:12 - loss: 4.9782 - accuracy: 0.1234jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  50/6000 [..............................] - ETA: 2:11 - loss: 5.0608 - accuracy: 0.1200jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  53/6000 [..............................] - ETA: 2:11 - loss: 5.0558 - accuracy: 0.1321jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  56/6000 [..............................] - ETA: 2:11 - loss: 5.0307 - accuracy: 0.1357jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  59/6000 [..............................] - ETA: 2:11 - loss: 4.9910 - accuracy: 0.1458jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  62/6000 [..............................] - ETA: 2:11 - loss: 5.0705 - accuracy: 0.1468jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  65/6000 [..............................] - ETA: 2:10 - loss: 5.1629 - accuracy: 0.1492jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  68/6000 [..............................] - ETA: 2:10 - loss: 5.3161 - accuracy: 0.1485jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  71/6000 [..............................] - ETA: 2:10 - loss: 5.4431 - accuracy: 0.1479jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  74/6000 [..............................] - ETA: 2:09 - loss: 5.5570 - accuracy: 0.1459jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  77/6000 [..............................] - ETA: 2:10 - loss: 5.6686 - accuracy: 0.1481jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  79/6000 [..............................] - ETA: 2:11 - loss: 5.7523 - accuracy: 0.1468jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  81/6000 [..............................] - ETA: 2:11 - loss: 5.8118 - accuracy: 0.1469jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  84/6000 [..............................] - ETA: 2:12 - loss: 5.7821 - accuracy: 0.1488jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  87/6000 [..............................] - ETA: 2:11 - loss: 5.7708 - accuracy: 0.1506jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  90/6000 [..............................] - ETA: 2:12 - loss: 5.7873 - accuracy: 0.1522jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  93/6000 [..............................] - ETA: 2:12 - loss: 6.1355 - accuracy: 0.1516jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  95/6000 [..............................] - ETA: 2:13 - loss: 6.1900 - accuracy: 0.1537jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "  97/6000 [..............................] - ETA: 2:13 - loss: 6.3610 - accuracy: 0.1557jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 100/6000 [..............................] - ETA: 2:13 - loss: 6.5614 - accuracy: 0.1560jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 103/6000 [..............................] - ETA: 2:13 - loss: 6.6728 - accuracy: 0.1553jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 105/6000 [..............................] - ETA: 2:13 - loss: 6.7866 - accuracy: 0.1533jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 108/6000 [..............................] - ETA: 2:13 - loss: 6.9323 - accuracy: 0.1537jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 111/6000 [..............................] - ETA: 2:13 - loss: 7.1568 - accuracy: 0.1514jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 114/6000 [..............................] - ETA: 2:13 - loss: 7.5058 - accuracy: 0.1500jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 117/6000 [..............................] - ETA: 2:13 - loss: 7.7385 - accuracy: 0.1513jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 120/6000 [..............................] - ETA: 2:13 - loss: 8.0032 - accuracy: 0.1483jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 123/6000 [..............................] - ETA: 2:13 - loss: 8.0079 - accuracy: 0.1472jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 124/6000 [..............................] - ETA: 2:18 - loss: 8.0304 - accuracy: 0.1476jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 125/6000 [..............................] - ETA: 2:20 - loss: 8.0500 - accuracy: 0.1464jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 127/6000 [..............................] - ETA: 2:20 - loss: 8.0889 - accuracy: 0.1457jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 129/6000 [..............................] - ETA: 2:21 - loss: 8.1375 - accuracy: 0.1442jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 131/6000 [..............................] - ETA: 2:22 - loss: 8.1744 - accuracy: 0.1420jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 133/6000 [..............................] - ETA: 2:23 - loss: 8.2632 - accuracy: 0.1398jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 135/6000 [..............................] - ETA: 2:23 - loss: 8.2801 - accuracy: 0.1415jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 137/6000 [..............................] - ETA: 2:24 - loss: 8.3410 - accuracy: 0.1409jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 139/6000 [..............................] - ETA: 2:25 - loss: 8.4261 - accuracy: 0.1388jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 142/6000 [..............................] - ETA: 2:25 - loss: 8.4629 - accuracy: 0.1408jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 144/6000 [..............................] - ETA: 2:25 - loss: 8.5085 - accuracy: 0.1424jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 146/6000 [..............................] - ETA: 2:25 - loss: 8.5649 - accuracy: 0.1418jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 148/6000 [..............................] - ETA: 2:26 - loss: 8.6261 - accuracy: 0.1412jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 151/6000 [..............................] - ETA: 2:25 - loss: 8.6369 - accuracy: 0.1411jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 154/6000 [..............................] - ETA: 2:25 - loss: 8.6233 - accuracy: 0.1396jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 156/6000 [..............................] - ETA: 2:26 - loss: 8.6024 - accuracy: 0.1404jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 158/6000 [..............................] - ETA: 2:27 - loss: 8.5741 - accuracy: 0.1411jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 159/6000 [..............................] - ETA: 2:30 - loss: 8.5647 - accuracy: 0.1415jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 161/6000 [..............................] - ETA: 2:30 - loss: 8.5542 - accuracy: 0.1410jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 164/6000 [..............................] - ETA: 2:30 - loss: 8.6084 - accuracy: 0.1402jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 166/6000 [..............................] - ETA: 2:30 - loss: 8.6528 - accuracy: 0.1386jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      "jacobian shape: 8 TensorShape([10, 5, 5, 1, 16])\n",
      " 168/6000 [..............................] - ETA: 2:31 - loss: 8.6728 - accuracy: 0.1381"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train model with Keras\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dpsgd:\n\u001b[0;32m---> 50\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# , early_stopping_callback\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_data, train_labels,\n\u001b[1;32m     57\u001b[0m             epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     58\u001b[0m             validation_data\u001b[38;5;241m=\u001b[39m(test_data, test_labels),\n\u001b[1;32m     59\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     60\u001b[0m             callbacks \u001b[38;5;241m=\u001b[39m [model_checkpoint_callback], workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# , early_stopping_callback\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/engine/training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1690\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1691\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1693\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m-> 1170\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseen, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/utils/generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    293\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[0;32m--> 296\u001b[0m     io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    297\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/utils/io_utils.py:79\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     77\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m         sys\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mwrite(message)\n\u001b[1;32m     80\u001b[0m     sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/ipykernel/iostream.py:648\u001b[0m, in \u001b[0;36mOutStream.write\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flush)\n\u001b[1;32m    647\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schedule_flush()\n\u001b[1;32m    650\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(string)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/ipykernel/iostream.py:545\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_schedule_in_thread\u001b[39m():\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_io_loop\u001b[39m.\u001b[39mcall_later(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflush_interval, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flush)\n\u001b[0;32m--> 545\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpub_thread\u001b[39m.\u001b[39;49mschedule(_schedule_in_thread)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/ipykernel/iostream.py:251\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_events\u001b[39m.\u001b[39mappend(f)\n\u001b[1;32m    250\u001b[0m     \u001b[39m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_pipe\u001b[39m.\u001b[39;49msend(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     f()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/zmq/sugar/socket.py:696\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    689\u001b[0m         data \u001b[39m=\u001b[39m zmq\u001b[39m.\u001b[39mFrame(\n\u001b[1;32m    690\u001b[0m             data,\n\u001b[1;32m    691\u001b[0m             track\u001b[39m=\u001b[39mtrack,\n\u001b[1;32m    692\u001b[0m             copy\u001b[39m=\u001b[39mcopy \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    693\u001b[0m             copy_threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_threshold,\n\u001b[1;32m    694\u001b[0m         )\n\u001b[1;32m    695\u001b[0m     data\u001b[39m.\u001b[39mgroup \u001b[39m=\u001b[39m group\n\u001b[0;32m--> 696\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(data, flags\u001b[39m=\u001b[39;49mflags, copy\u001b[39m=\u001b[39;49mcopy, track\u001b[39m=\u001b[39;49mtrack)\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check the training accuracy\n",
    "dpsgd = True\n",
    "parameter_list = {}\n",
    "accuracies = {}\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\"]: #, \"ALLnoise+mnist+tied_bias\", \"mnist+tied_bias+noise_input\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+untied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        # optimizer = DPKerasSGDOptimizer(\n",
    "        #     l2_norm_clip=l2_norm_clip,\n",
    "        #     noise_multiplier=noise_multiplier,\n",
    "        #     gradient_accumulation_steps = 4,\n",
    "        #     learning_rate=lr_schedule)\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=lr_schedule)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule_no_dp)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: 'TiedBiasLayer'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdp_data/accuracies2.0_3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/saving/pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m     model \u001b[39m=\u001b[39m saving_lib\u001b[39m.\u001b[39mload_model(filepath, safe_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/saving/pickle_utils.py:46\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     40\u001b[0m         f\u001b[39m.\u001b[39mwrite(serialized_model)\n\u001b[1;32m     41\u001b[0m     \u001b[39m# When loading, direct import will work for most custom objects\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# though it will require get_config() to be implemented.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[39m# Some custom objects (e.g. an activation in a Dense layer,\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39m# serialized as a string by Dense.get_config()) will require\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[39m# a custom_object_scope.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     model \u001b[39m=\u001b[39m saving_lib\u001b[39m.\u001b[39;49mload_model(filepath, safe_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     48\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/saving/saving_lib.py:277\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    274\u001b[0m             asset_store\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    276\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/saving/saving_lib.py:242\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 242\u001b[0m     model \u001b[39m=\u001b[39m deserialize_keras_object(\n\u001b[1;32m    243\u001b[0m         config_dict, custom_objects, safe_mode\u001b[39m=\u001b[39;49msafe_mode\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    246\u001b[0m all_filenames \u001b[39m=\u001b[39m zf\u001b[39m.\u001b[39mnamelist()\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m _VARS_FNAME \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/saving/serialization_lib.py:502\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m safe_mode_scope \u001b[39m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[1;32m    501\u001b[0m \u001b[39mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[0;32m--> 502\u001b[0m     instance \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_config(inner_config)\n\u001b[1;32m    503\u001b[0m     build_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbuild_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m build_config:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/engine/sequential.py:476\u001b[0m, in \u001b[0;36mSequential.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    474\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(name\u001b[39m=\u001b[39mname)\n\u001b[1;32m    475\u001b[0m \u001b[39mfor\u001b[39;00m layer_config \u001b[39min\u001b[39;00m layer_configs:\n\u001b[0;32m--> 476\u001b[0m     layer \u001b[39m=\u001b[39m layer_module\u001b[39m.\u001b[39;49mdeserialize(\n\u001b[1;32m    477\u001b[0m         layer_config, custom_objects\u001b[39m=\u001b[39;49mcustom_objects\n\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m     model\u001b[39m.\u001b[39madd(layer)\n\u001b[1;32m    481\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    482\u001b[0m     \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39minputs\n\u001b[1;32m    483\u001b[0m     \u001b[39mand\u001b[39;00m build_input_shape\n\u001b[1;32m    484\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(build_input_shape, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m))\n\u001b[1;32m    485\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/layers/serialization.py:265\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects, use_legacy_format)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_serialization\u001b[39m.\u001b[39mdeserialize_keras_object(\n\u001b[1;32m    258\u001b[0m         config,\n\u001b[1;32m    259\u001b[0m         module_objects\u001b[39m=\u001b[39mLOCAL\u001b[39m.\u001b[39mALL_OBJECTS,\n\u001b[1;32m    260\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    261\u001b[0m         printable_module_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    262\u001b[0m     )\n\u001b[1;32m    264\u001b[0m \u001b[39m# To be replaced by new serialization_lib\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_serialization\u001b[39m.\u001b[39;49mdeserialize_keras_object(\n\u001b[1;32m    266\u001b[0m     config,\n\u001b[1;32m    267\u001b[0m     module_objects\u001b[39m=\u001b[39;49mLOCAL\u001b[39m.\u001b[39;49mALL_OBJECTS,\n\u001b[1;32m    268\u001b[0m     custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[1;32m    269\u001b[0m     printable_module_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlayer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    270\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/saving/legacy/serialization.py:486\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(identifier, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    484\u001b[0m     \u001b[39m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     config \u001b[39m=\u001b[39m identifier\n\u001b[0;32m--> 486\u001b[0m     (\u001b[39mcls\u001b[39m, cls_config) \u001b[39m=\u001b[39m class_and_config_for_serialized_keras_object(\n\u001b[1;32m    487\u001b[0m         config, module_objects, custom_objects, printable_module_name\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[39m# If this object has already been loaded (i.e. it's shared between\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[39m# multiple objects), return the already-loaded object.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     shared_object_id \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(SHARED_OBJECT_KEY)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dpsgd/lib/python3.11/site-packages/keras/saving/legacy/serialization.py:368\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m object_registration\u001b[39m.\u001b[39mget_registered_object(\n\u001b[1;32m    365\u001b[0m     class_name, custom_objects, module_objects\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    369\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown \u001b[39m\u001b[39m{\u001b[39;00mprintable_module_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand that this object is included in the scope. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#registering_the_custom_object for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m cls_config \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    377\u001b[0m \u001b[39m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[39m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: 'TiedBiasLayer'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_noise(x, clip, noise_multiplier):\n",
    "    def clip_features(x):\n",
    "        return tf.clip_by_global_norm([x], clip)[0][0]\n",
    "    clipped_features = tf.map_fn(clip_features, x)\n",
    "    # Add noise to summed gradients.\n",
    "    noise_stddev = clip * noise_multiplier\n",
    "    noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "    return clipped_features + noise\n",
    "\n",
    "noisy_train_data = tf.map_fn(lambda x: clip_noise(x, l2_norm_clip, noise_multiplier), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training accuracy\n",
    "\n",
    "parameter_list = {}\n",
    "model_type = \"noisy_permanent_input\"\n",
    "dpsgd = False\n",
    "file_name = model_type + \"_accuracy\"\n",
    "model = build_models(\"mnist+untied_bias\")\n",
    "if dpsgd:\n",
    "    optimizer = FixedDPKerasSGDOptimizer(\n",
    "        batch_size = batch_size,\n",
    "        num_parameters = num_parameters,\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        var_list = model.trainable_variables,\n",
    "        #num_microbatches=batch_size//microbatch_size,\n",
    "        microbatch_size = microbatch_size,\n",
    "        learning_rate=learning_rate)\n",
    "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with Keras\n",
    "checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=checkpoint_filepath,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_accuracy',\n",
    "  mode='max',\n",
    "  save_best_only=True)\n",
    "#early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "#    mode='auto', baseline=None, restore_best_weights=True\n",
    "#)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "# Train model with Keras\n",
    "if dpsgd:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=microbatch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "else:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=batch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "\n",
    "#evaluated_result = model.evaluate(\n",
    "#    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "#    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "#    return_dict=True)\n",
    "if model_type not in accuracies.keys():\n",
    "    accuracies[model_type] = []\n",
    "accuracies[model_type].append(history)\n",
    "#accuracies[model_type].append(evaluated_result[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies = dict()\n",
    "for key in accuracies.keys():   \n",
    "    processed_accuracies[key] = accuracies[key][0].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['val_accuracy'])\n",
    "#plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"dp_data/results/accuracies_%.1f_%.1f_%.1f_%d\"%(learning_rate, noise_multiplier, l2_norm_clip, batch_size),\"wb\")\n",
    "pickle.dump(processed_accuracies, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the step-epsilon curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "try:\n",
    "    with open(\"dp_data/noise_epsilon_step\") as f:\n",
    "        epsilons = pickle.load(f)\n",
    "except:\n",
    "    training_data_size = 60000\n",
    "    max_step = 25000\n",
    "    epsilons = [[] for i in range(10)]\n",
    "    for noise_multiplier in range(0, 10, 1):\n",
    "        print(\"noise_multiplier:\", noise_multiplier)\n",
    "        for step in range(1, max_step, 10):\n",
    "            epsilons[noise_multiplier].append(compute_epsilon(step, batch_size/training_data_size, noise_multiplier))\n",
    "    file = open(\"dp_data/noise_epsilon_step_%d\"%batch_size,\"wb\")\n",
    "    pickle.dump(epsilons, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tied Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tied bias\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\", \"ALLnoise+mnist+tied_bias\",\"mnist+tied_bias\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+tied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=learning_rate)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "for key in parameter_list.keys():\n",
    "    line1, = ax.plot(parameter_list[key],accuracies[key],  label=key)\n",
    "    line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee\n",
    "import pickle\n",
    "result = open(\"result\",\"wb\")\n",
    "pickle.dump(accuracies, result)\n",
    "pickle.dump(parameter_list, result)\n",
    "result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(\"result\", \"rb\")\n",
    "accuracies = pickle.load(result)\n",
    "parameter_list = pickle.load(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "line1, = ax.plot(parameter_list[\"linear\"],accuracies[\"linear\"],  label='Linear')\n",
    "line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "line2, = ax.plot(parameter_list[\"bias\"],  accuracies[\"bias\"], label='Bias')\n",
    "line2.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "a = [tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32), tf.constant([3., 4, 5], shape=(3,), dtype=tf.float32), tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32)]\n",
    "b = tf.ones_like(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linalg.global_norm(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_global_norm(a, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('dpsgd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f09f6991b5e558a26792a95a4fa1ecc05fb6c2026e4b74200355c105e722a54b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
