{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: 2021/03/14\n",
    "# Usage: Dp analysis of mnist neural network.\n",
    "# Detail: Check the notion notes in HE-DP project.\n",
    "\n",
    "from dpsgd_keras_slow import *\n",
    "\n",
    "dpsgd = False # add dp noise or not \n",
    "learning_rate = 0.1\n",
    "noise_multiplier = 4\n",
    "l2_norm_clip = 3\n",
    "batch_size = 512\n",
    "epochs = 120\n",
    "microbatch_size = 64\n",
    "num_parameters = 0\n",
    "privacy_budget = []\n",
    "delta = 1e-5  # it is recommended to use delta~=1/dataset_size\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n",
    "  test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturnbing the input dataset, and collect the accuracy-step\n",
    "class GaussianNoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, l2_norm_clip, noise_multiplier, *args, **kwargs):\n",
    "        super(GaussianNoiseLayer, self).__init__(*args, **kwargs)\n",
    "        self._l2_norm_clip = l2_norm_clip\n",
    "        self._noise_multiplier = noise_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Clip gradients to given l2_norm_clip.\n",
    "        def clip_features(x):\n",
    "            return tf.clip_by_global_norm([x], self._l2_norm_clip)[0][0]\n",
    "\n",
    "        clipped_features = tf.map_fn(clip_features, x)\n",
    "        \n",
    "        # Add noise to summed gradients.\n",
    "        noise_stddev = self._l2_norm_clip * self._noise_multiplier\n",
    "        noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "        return clipped_features + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.clip_by_global_norm([tf.Variable([[2,3],[4,5]],shape=[2,2],dtype=tf.float32)], 4.4)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "def build_models(noise_layer_name):\n",
    "    if noise_layer_name ==\"mnist+untied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])        \n",
    "    elif noise_layer_name == \"mnist+tied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])   \n",
    "    elif noise_layer_name == \"mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])  \n",
    "    elif noise_layer_name == \"sphinx+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"sphinx+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(name=\"Considered2\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            BiasLayer(name=\"Considered8\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(name=\"Considered7\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(name=\"Considered8\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10\":\n",
    "        pass\n",
    "    else:\n",
    "        model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_layer (Gaussi (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "bias_layer (BiasLayer)       (None, 24, 24, 16)        9216      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "bias_layer_1 (BiasLayer)     (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_2 (BiasLayer)     (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_3 (BiasLayer)     (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 43,750\n",
      "Trainable params: 43,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist+untied_bias+noise_input\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "bias_layer_4 (BiasLayer)     (None, 24, 24, 16)        9216      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "bias_layer_5 (BiasLayer)     (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_6 (BiasLayer)     (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_7 (BiasLayer)     (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 43,750\n",
      "Trainable params: 43,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist+untied_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "tied_bias_layer (TiedBiasLay (None, 24, 24, 16)        16        \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_1 (TiedBiasL (None, 8, 8, 16)          16        \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_8 (BiasLayer)     (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_9 (BiasLayer)     (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 33,542\n",
      "Trainable params: 33,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist+tied_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 16)        400       \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 16)          6400      \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               25600     \n",
      "_________________________________________________________________\n",
      "bias_layer_10 (BiasLayer)    (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "bias_layer_11 (BiasLayer)    (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 33,510\n",
      "Trainable params: 33,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"mnist\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main_simple(unused_argv):\n",
    "logging.set_verbosity(logging.INFO)\n",
    "if dpsgd and batch_size % microbatch_size != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // microbatch_size // 1000,\n",
    "    decay_rate=0.998)\n",
    "\n",
    "\n",
    "lr_schedule_no_dp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // batch_size // 1000,\n",
    "    decay_rate=0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "938/938 [==============================] - 146s 121ms/step - loss: 1.7767 - accuracy: 0.4275 - val_loss: 0.5725 - val_accuracy: 0.8103\n",
      "Epoch 2/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.5577 - accuracy: 0.8223 - val_loss: 0.4601 - val_accuracy: 0.8691\n",
      "Epoch 3/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4951 - accuracy: 0.8613 - val_loss: 0.4524 - val_accuracy: 0.8844\n",
      "Epoch 4/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4953 - accuracy: 0.8752 - val_loss: 0.4589 - val_accuracy: 0.8908\n",
      "Epoch 5/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4917 - accuracy: 0.8823 - val_loss: 0.4771 - val_accuracy: 0.8895\n",
      "Epoch 6/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.5060 - accuracy: 0.8870 - val_loss: 0.4654 - val_accuracy: 0.8980\n",
      "Epoch 7/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.5074 - accuracy: 0.8918 - val_loss: 0.4621 - val_accuracy: 0.8975\n",
      "Epoch 8/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.5025 - accuracy: 0.8931 - val_loss: 0.4579 - val_accuracy: 0.9028\n",
      "Epoch 9/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.4923 - accuracy: 0.8993 - val_loss: 0.4502 - val_accuracy: 0.9060\n",
      "Epoch 10/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.4919 - accuracy: 0.9013 - val_loss: 0.4401 - val_accuracy: 0.9095\n",
      "Epoch 11/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.4807 - accuracy: 0.9046 - val_loss: 0.4233 - val_accuracy: 0.9106\n",
      "Epoch 12/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.4719 - accuracy: 0.9049 - val_loss: 0.4251 - val_accuracy: 0.9144\n",
      "Epoch 13/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.4806 - accuracy: 0.9059 - val_loss: 0.4327 - val_accuracy: 0.9128\n",
      "Epoch 14/120\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.4589 - accuracy: 0.9087 - val_loss: 0.4338 - val_accuracy: 0.9149\n",
      "Epoch 15/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.4558 - accuracy: 0.9110 - val_loss: 0.4080 - val_accuracy: 0.9211\n",
      "Epoch 16/120\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.4361 - accuracy: 0.9148 - val_loss: 0.4060 - val_accuracy: 0.9194\n",
      "Epoch 17/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4390 - accuracy: 0.9152 - val_loss: 0.3924 - val_accuracy: 0.9199\n",
      "Epoch 18/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.4329 - accuracy: 0.9179 - val_loss: 0.4006 - val_accuracy: 0.9198\n",
      "Epoch 19/120\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.4174 - accuracy: 0.9192 - val_loss: 0.3825 - val_accuracy: 0.9236\n",
      "Epoch 20/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.4154 - accuracy: 0.9181 - val_loss: 0.3842 - val_accuracy: 0.9232\n",
      "Epoch 21/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.4057 - accuracy: 0.9207 - val_loss: 0.3633 - val_accuracy: 0.9272\n",
      "Epoch 22/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4209 - accuracy: 0.9195 - val_loss: 0.3686 - val_accuracy: 0.9256\n",
      "Epoch 23/120\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.4212 - accuracy: 0.9185 - val_loss: 0.3898 - val_accuracy: 0.9240\n",
      "Epoch 24/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4141 - accuracy: 0.9203 - val_loss: 0.3656 - val_accuracy: 0.9286\n",
      "Epoch 25/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3947 - accuracy: 0.9234 - val_loss: 0.3539 - val_accuracy: 0.9295\n",
      "Epoch 26/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4085 - accuracy: 0.9232 - val_loss: 0.3503 - val_accuracy: 0.9309\n",
      "Epoch 27/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4083 - accuracy: 0.9223 - val_loss: 0.3648 - val_accuracy: 0.9309\n",
      "Epoch 28/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.4090 - accuracy: 0.9245 - val_loss: 0.3897 - val_accuracy: 0.9296\n",
      "Epoch 29/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.4045 - accuracy: 0.9254 - val_loss: 0.3926 - val_accuracy: 0.9273\n",
      "Epoch 30/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.4091 - accuracy: 0.9274 - val_loss: 0.3964 - val_accuracy: 0.9276\n",
      "Epoch 31/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.4148 - accuracy: 0.9249 - val_loss: 0.3957 - val_accuracy: 0.9299\n",
      "Epoch 32/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.4217 - accuracy: 0.9268 - val_loss: 0.4022 - val_accuracy: 0.9299\n",
      "Epoch 33/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.4083 - accuracy: 0.9283 - val_loss: 0.3869 - val_accuracy: 0.9302\n",
      "Epoch 34/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.4323 - accuracy: 0.9259 - val_loss: 0.3710 - val_accuracy: 0.9325\n",
      "Epoch 35/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4076 - accuracy: 0.9279 - val_loss: 0.3846 - val_accuracy: 0.9310\n",
      "Epoch 36/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.4250 - accuracy: 0.9259 - val_loss: 0.3827 - val_accuracy: 0.9327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4087 - accuracy: 0.9285 - val_loss: 0.3589 - val_accuracy: 0.9350\n",
      "Epoch 38/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4086 - accuracy: 0.9287 - val_loss: 0.3809 - val_accuracy: 0.9336\n",
      "Epoch 39/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.4113 - accuracy: 0.9292 - val_loss: 0.3558 - val_accuracy: 0.9359\n",
      "Epoch 40/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3931 - accuracy: 0.9312 - val_loss: 0.3711 - val_accuracy: 0.9355\n",
      "Epoch 41/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4037 - accuracy: 0.9333 - val_loss: 0.3544 - val_accuracy: 0.9383\n",
      "Epoch 42/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.4074 - accuracy: 0.9304 - val_loss: 0.3651 - val_accuracy: 0.9369\n",
      "Epoch 43/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.4047 - accuracy: 0.9327 - val_loss: 0.3659 - val_accuracy: 0.9383\n",
      "Epoch 44/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3891 - accuracy: 0.9328 - val_loss: 0.3644 - val_accuracy: 0.9369\n",
      "Epoch 45/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3861 - accuracy: 0.9328 - val_loss: 0.3632 - val_accuracy: 0.9377\n",
      "Epoch 46/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.4090 - accuracy: 0.9319 - val_loss: 0.3615 - val_accuracy: 0.9386\n",
      "Epoch 47/120\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.3881 - accuracy: 0.9343 - val_loss: 0.3446 - val_accuracy: 0.9415\n",
      "Epoch 48/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.3864 - accuracy: 0.9358 - val_loss: 0.3418 - val_accuracy: 0.9393\n",
      "Epoch 49/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.3801 - accuracy: 0.9350 - val_loss: 0.3298 - val_accuracy: 0.9415\n",
      "Epoch 50/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3789 - accuracy: 0.9380 - val_loss: 0.3372 - val_accuracy: 0.9409\n",
      "Epoch 51/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.3877 - accuracy: 0.9361 - val_loss: 0.3450 - val_accuracy: 0.9423\n",
      "Epoch 52/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.4006 - accuracy: 0.9335 - val_loss: 0.3352 - val_accuracy: 0.9419\n",
      "Epoch 53/120\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.4033 - accuracy: 0.9335 - val_loss: 0.3383 - val_accuracy: 0.9436\n",
      "Epoch 54/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3783 - accuracy: 0.9373 - val_loss: 0.3300 - val_accuracy: 0.9417\n",
      "Epoch 55/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3701 - accuracy: 0.9382 - val_loss: 0.3482 - val_accuracy: 0.9411\n",
      "Epoch 56/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3733 - accuracy: 0.9373 - val_loss: 0.3526 - val_accuracy: 0.9406\n",
      "Epoch 57/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3824 - accuracy: 0.9372 - val_loss: 0.3467 - val_accuracy: 0.9405\n",
      "Epoch 58/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3632 - accuracy: 0.9384 - val_loss: 0.3451 - val_accuracy: 0.9415\n",
      "Epoch 59/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3749 - accuracy: 0.9387 - val_loss: 0.3495 - val_accuracy: 0.9433\n",
      "Epoch 60/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3763 - accuracy: 0.9383 - val_loss: 0.3464 - val_accuracy: 0.9421\n",
      "Epoch 61/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3547 - accuracy: 0.9392 - val_loss: 0.3491 - val_accuracy: 0.9417\n",
      "Epoch 62/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3824 - accuracy: 0.9372 - val_loss: 0.3404 - val_accuracy: 0.9427\n",
      "Epoch 63/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.3711 - accuracy: 0.9382 - val_loss: 0.3299 - val_accuracy: 0.9428\n",
      "Epoch 64/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3559 - accuracy: 0.9394 - val_loss: 0.3220 - val_accuracy: 0.9447\n",
      "Epoch 65/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.3515 - accuracy: 0.9425 - val_loss: 0.3266 - val_accuracy: 0.9442\n",
      "Epoch 66/120\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.3425 - accuracy: 0.9435 - val_loss: 0.3236 - val_accuracy: 0.9434\n",
      "Epoch 67/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3503 - accuracy: 0.9418 - val_loss: 0.3245 - val_accuracy: 0.9434\n",
      "Epoch 68/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3363 - accuracy: 0.9419 - val_loss: 0.3311 - val_accuracy: 0.9440\n",
      "Epoch 69/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3499 - accuracy: 0.9404 - val_loss: 0.3271 - val_accuracy: 0.9455\n",
      "Epoch 70/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3294 - accuracy: 0.9436 - val_loss: 0.3075 - val_accuracy: 0.9480\n",
      "Epoch 71/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3314 - accuracy: 0.9430 - val_loss: 0.3008 - val_accuracy: 0.9481\n",
      "Epoch 72/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3444 - accuracy: 0.9412 - val_loss: 0.3147 - val_accuracy: 0.9479\n",
      "Epoch 73/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3544 - accuracy: 0.9416 - val_loss: 0.3118 - val_accuracy: 0.9475\n",
      "Epoch 74/120\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.3386 - accuracy: 0.9416 - val_loss: 0.3087 - val_accuracy: 0.9491\n",
      "Epoch 75/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3443 - accuracy: 0.9437 - val_loss: 0.3051 - val_accuracy: 0.9492\n",
      "Epoch 76/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3294 - accuracy: 0.9439 - val_loss: 0.3120 - val_accuracy: 0.9476\n",
      "Epoch 77/120\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.3328 - accuracy: 0.9444 - val_loss: 0.3120 - val_accuracy: 0.9469\n",
      "Epoch 78/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3325 - accuracy: 0.9438 - val_loss: 0.3082 - val_accuracy: 0.9485\n",
      "Epoch 79/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3339 - accuracy: 0.9445 - val_loss: 0.3104 - val_accuracy: 0.9480\n",
      "Epoch 80/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3369 - accuracy: 0.9437 - val_loss: 0.3081 - val_accuracy: 0.9477\n",
      "Epoch 81/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3269 - accuracy: 0.9454 - val_loss: 0.3110 - val_accuracy: 0.9478\n",
      "Epoch 82/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.3380 - accuracy: 0.9455 - val_loss: 0.3087 - val_accuracy: 0.9501\n",
      "Epoch 83/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3270 - accuracy: 0.9466 - val_loss: 0.3093 - val_accuracy: 0.9494\n",
      "Epoch 84/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3319 - accuracy: 0.9459 - val_loss: 0.3035 - val_accuracy: 0.9501\n",
      "Epoch 85/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.3166 - accuracy: 0.9464 - val_loss: 0.2996 - val_accuracy: 0.9499\n",
      "Epoch 86/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3308 - accuracy: 0.9467 - val_loss: 0.2976 - val_accuracy: 0.9510\n",
      "Epoch 87/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3303 - accuracy: 0.9465 - val_loss: 0.3038 - val_accuracy: 0.9514\n",
      "Epoch 88/120\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.3367 - accuracy: 0.9452 - val_loss: 0.3046 - val_accuracy: 0.9512\n",
      "Epoch 89/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3444 - accuracy: 0.9441 - val_loss: 0.3023 - val_accuracy: 0.9509\n",
      "Epoch 90/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3282 - accuracy: 0.9453 - val_loss: 0.3012 - val_accuracy: 0.9505\n",
      "Epoch 91/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.3263 - accuracy: 0.9459 - val_loss: 0.2906 - val_accuracy: 0.9524\n",
      "Epoch 92/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3175 - accuracy: 0.9467 - val_loss: 0.2817 - val_accuracy: 0.9533\n",
      "Epoch 93/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3274 - accuracy: 0.9454 - val_loss: 0.2870 - val_accuracy: 0.9526\n",
      "Epoch 94/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3181 - accuracy: 0.9461 - val_loss: 0.2861 - val_accuracy: 0.9534\n",
      "Epoch 95/120\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.3234 - accuracy: 0.9469 - val_loss: 0.2857 - val_accuracy: 0.9528\n",
      "Epoch 96/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3169 - accuracy: 0.9460 - val_loss: 0.2849 - val_accuracy: 0.9533\n",
      "Epoch 97/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3115 - accuracy: 0.9476 - val_loss: 0.2904 - val_accuracy: 0.9535\n",
      "Epoch 98/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3181 - accuracy: 0.9466 - val_loss: 0.2846 - val_accuracy: 0.9538\n",
      "Epoch 99/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3104 - accuracy: 0.9474 - val_loss: 0.2853 - val_accuracy: 0.9531\n",
      "Epoch 100/120\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.3316 - accuracy: 0.9459 - val_loss: 0.2836 - val_accuracy: 0.9535\n",
      "Epoch 101/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3124 - accuracy: 0.9474 - val_loss: 0.2893 - val_accuracy: 0.9514\n",
      "Epoch 102/120\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.3044 - accuracy: 0.9486 - val_loss: 0.2851 - val_accuracy: 0.9524\n",
      "Epoch 103/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.3082 - accuracy: 0.9476 - val_loss: 0.2830 - val_accuracy: 0.9524\n",
      "Epoch 104/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3087 - accuracy: 0.9480 - val_loss: 0.2797 - val_accuracy: 0.9529\n",
      "Epoch 105/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.2955 - accuracy: 0.9490 - val_loss: 0.2798 - val_accuracy: 0.9528\n",
      "Epoch 106/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3153 - accuracy: 0.9452 - val_loss: 0.2810 - val_accuracy: 0.9536\n",
      "Epoch 107/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.2979 - accuracy: 0.9486 - val_loss: 0.2783 - val_accuracy: 0.9532\n",
      "Epoch 108/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.2999 - accuracy: 0.9488 - val_loss: 0.2783 - val_accuracy: 0.9530\n",
      "Epoch 109/120\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.3021 - accuracy: 0.9494 - val_loss: 0.2737 - val_accuracy: 0.9537\n",
      "Epoch 110/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.2946 - accuracy: 0.9496 - val_loss: 0.2774 - val_accuracy: 0.9534\n",
      "Epoch 111/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.3077 - accuracy: 0.9485 - val_loss: 0.2753 - val_accuracy: 0.9527\n",
      "Epoch 112/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.2964 - accuracy: 0.9494 - val_loss: 0.2769 - val_accuracy: 0.9525\n",
      "Epoch 113/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.3057 - accuracy: 0.9505 - val_loss: 0.2772 - val_accuracy: 0.9527\n",
      "Epoch 114/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.2953 - accuracy: 0.9505 - val_loss: 0.2809 - val_accuracy: 0.9521\n",
      "Epoch 115/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3102 - accuracy: 0.9484 - val_loss: 0.2783 - val_accuracy: 0.9522\n",
      "Epoch 116/120\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.3051 - accuracy: 0.9489 - val_loss: 0.2815 - val_accuracy: 0.9524\n",
      "Epoch 117/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.3053 - accuracy: 0.9483 - val_loss: 0.2833 - val_accuracy: 0.9540\n",
      "Epoch 118/120\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.2906 - accuracy: 0.9507 - val_loss: 0.2802 - val_accuracy: 0.9531\n",
      "Epoch 119/120\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.3063 - accuracy: 0.9484 - val_loss: 0.2791 - val_accuracy: 0.9533\n",
      "Epoch 120/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.2956 - accuracy: 0.9506 - val_loss: 0.2792 - val_accuracy: 0.9529\n",
      "Epoch 1/120\n",
      "compute the gradient\n",
      "[True, True, True, True, True, True, True, True]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "compute the gradient\n",
      "[True, True, True, True, True, True, True, True]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "938/938 [==============================] - 78s 80ms/step - loss: 1.7093 - accuracy: 0.4706 - val_loss: 0.6015 - val_accuracy: 0.8114\n",
      "Epoch 2/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.5866 - accuracy: 0.8183 - val_loss: 0.4846 - val_accuracy: 0.8629\n",
      "Epoch 3/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.5216 - accuracy: 0.8589 - val_loss: 0.4831 - val_accuracy: 0.8783\n",
      "Epoch 4/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.5152 - accuracy: 0.8729 - val_loss: 0.4809 - val_accuracy: 0.8844\n",
      "Epoch 5/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.5101 - accuracy: 0.8787 - val_loss: 0.4768 - val_accuracy: 0.8919\n",
      "Epoch 6/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.5107 - accuracy: 0.8849 - val_loss: 0.4886 - val_accuracy: 0.8940\n",
      "Epoch 7/120\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.5124 - accuracy: 0.8895 - val_loss: 0.4848 - val_accuracy: 0.8991\n",
      "Epoch 8/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.5076 - accuracy: 0.8928 - val_loss: 0.4681 - val_accuracy: 0.9020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4936 - accuracy: 0.8967 - val_loss: 0.4744 - val_accuracy: 0.9017\n",
      "Epoch 10/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4690 - accuracy: 0.9010 - val_loss: 0.4524 - val_accuracy: 0.9069\n",
      "Epoch 11/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4662 - accuracy: 0.9032 - val_loss: 0.4611 - val_accuracy: 0.9092\n",
      "Epoch 12/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4742 - accuracy: 0.9038 - val_loss: 0.4319 - val_accuracy: 0.9094\n",
      "Epoch 13/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4410 - accuracy: 0.9057 - val_loss: 0.4461 - val_accuracy: 0.9096\n",
      "Epoch 14/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4525 - accuracy: 0.9079 - val_loss: 0.4352 - val_accuracy: 0.9140\n",
      "Epoch 15/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4529 - accuracy: 0.9089 - val_loss: 0.4385 - val_accuracy: 0.9119\n",
      "Epoch 16/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4537 - accuracy: 0.9103 - val_loss: 0.4452 - val_accuracy: 0.9142\n",
      "Epoch 17/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4611 - accuracy: 0.9098 - val_loss: 0.4393 - val_accuracy: 0.9165\n",
      "Epoch 18/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4491 - accuracy: 0.9147 - val_loss: 0.4494 - val_accuracy: 0.9145\n",
      "Epoch 19/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4570 - accuracy: 0.9126 - val_loss: 0.4411 - val_accuracy: 0.9188\n",
      "Epoch 20/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4552 - accuracy: 0.9152 - val_loss: 0.4214 - val_accuracy: 0.9205\n",
      "Epoch 21/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4411 - accuracy: 0.9154 - val_loss: 0.4085 - val_accuracy: 0.9198\n",
      "Epoch 22/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4328 - accuracy: 0.9190 - val_loss: 0.3998 - val_accuracy: 0.9215\n",
      "Epoch 23/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4153 - accuracy: 0.9196 - val_loss: 0.3923 - val_accuracy: 0.9238\n",
      "Epoch 24/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4164 - accuracy: 0.9208 - val_loss: 0.3965 - val_accuracy: 0.9248\n",
      "Epoch 25/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3991 - accuracy: 0.9220 - val_loss: 0.3979 - val_accuracy: 0.9254\n",
      "Epoch 26/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4140 - accuracy: 0.9199 - val_loss: 0.3877 - val_accuracy: 0.9248\n",
      "Epoch 27/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4200 - accuracy: 0.9204 - val_loss: 0.3962 - val_accuracy: 0.9273\n",
      "Epoch 28/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4059 - accuracy: 0.9253 - val_loss: 0.3999 - val_accuracy: 0.9276\n",
      "Epoch 29/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4156 - accuracy: 0.9241 - val_loss: 0.3867 - val_accuracy: 0.9286\n",
      "Epoch 30/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4055 - accuracy: 0.9246 - val_loss: 0.3800 - val_accuracy: 0.9322\n",
      "Epoch 31/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4115 - accuracy: 0.9279 - val_loss: 0.3871 - val_accuracy: 0.9310\n",
      "Epoch 32/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.3995 - accuracy: 0.9279 - val_loss: 0.3764 - val_accuracy: 0.9327\n",
      "Epoch 33/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4244 - accuracy: 0.9259 - val_loss: 0.4015 - val_accuracy: 0.9321\n",
      "Epoch 34/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4202 - accuracy: 0.9287 - val_loss: 0.3859 - val_accuracy: 0.9323\n",
      "Epoch 35/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4032 - accuracy: 0.9302 - val_loss: 0.3700 - val_accuracy: 0.9366\n",
      "Epoch 36/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4095 - accuracy: 0.9302 - val_loss: 0.3825 - val_accuracy: 0.9343\n",
      "Epoch 37/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4090 - accuracy: 0.9308 - val_loss: 0.3797 - val_accuracy: 0.9349\n",
      "Epoch 38/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4034 - accuracy: 0.9295 - val_loss: 0.3755 - val_accuracy: 0.9355\n",
      "Epoch 39/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4117 - accuracy: 0.9298 - val_loss: 0.3807 - val_accuracy: 0.9360\n",
      "Epoch 40/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4197 - accuracy: 0.9314 - val_loss: 0.3801 - val_accuracy: 0.9344\n",
      "Epoch 41/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4097 - accuracy: 0.9319 - val_loss: 0.3818 - val_accuracy: 0.9360\n",
      "Epoch 42/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4165 - accuracy: 0.9314 - val_loss: 0.3828 - val_accuracy: 0.9368\n",
      "Epoch 43/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4140 - accuracy: 0.9338 - val_loss: 0.3675 - val_accuracy: 0.9365\n",
      "Epoch 44/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4155 - accuracy: 0.9329 - val_loss: 0.3696 - val_accuracy: 0.9380\n",
      "Epoch 45/120\n",
      "938/938 [==============================] - 74s 78ms/step - loss: 0.4165 - accuracy: 0.9323 - val_loss: 0.3669 - val_accuracy: 0.9389\n",
      "Epoch 46/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4177 - accuracy: 0.9318 - val_loss: 0.3798 - val_accuracy: 0.9371\n",
      "Epoch 47/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4012 - accuracy: 0.9340 - val_loss: 0.3698 - val_accuracy: 0.9377\n",
      "Epoch 48/120\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.4047 - accuracy: 0.9346 - val_loss: 0.3650 - val_accuracy: 0.9366\n",
      "Epoch 49/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4021 - accuracy: 0.9344 - val_loss: 0.3716 - val_accuracy: 0.9383\n",
      "Epoch 50/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4246 - accuracy: 0.9331 - val_loss: 0.3796 - val_accuracy: 0.9373\n",
      "Epoch 51/120\n",
      "938/938 [==============================] - 75s 79ms/step - loss: 0.4234 - accuracy: 0.9324 - val_loss: 0.3630 - val_accuracy: 0.9384\n",
      "Epoch 52/120\n",
      "938/938 [==============================] - 76s 82ms/step - loss: 0.4079 - accuracy: 0.9344 - val_loss: 0.3775 - val_accuracy: 0.9380\n",
      "Epoch 53/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4256 - accuracy: 0.9317 - val_loss: 0.3699 - val_accuracy: 0.9378\n",
      "Epoch 54/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4209 - accuracy: 0.9310 - val_loss: 0.3694 - val_accuracy: 0.9367\n",
      "Epoch 55/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3900 - accuracy: 0.9352 - val_loss: 0.3527 - val_accuracy: 0.9392\n",
      "Epoch 56/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3880 - accuracy: 0.9350 - val_loss: 0.3552 - val_accuracy: 0.9394\n",
      "Epoch 57/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3788 - accuracy: 0.9368 - val_loss: 0.3567 - val_accuracy: 0.9404\n",
      "Epoch 58/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.3955 - accuracy: 0.9339 - val_loss: 0.3637 - val_accuracy: 0.9390\n",
      "Epoch 59/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3831 - accuracy: 0.9378 - val_loss: 0.3616 - val_accuracy: 0.9401\n",
      "Epoch 60/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3984 - accuracy: 0.9368 - val_loss: 0.3696 - val_accuracy: 0.9384\n",
      "Epoch 61/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4078 - accuracy: 0.9363 - val_loss: 0.3642 - val_accuracy: 0.9390\n",
      "Epoch 62/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4027 - accuracy: 0.9378 - val_loss: 0.3730 - val_accuracy: 0.9374\n",
      "Epoch 63/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.4080 - accuracy: 0.9346 - val_loss: 0.3706 - val_accuracy: 0.9385\n",
      "Epoch 64/120\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.3940 - accuracy: 0.9361 - val_loss: 0.3773 - val_accuracy: 0.9396\n",
      "Epoch 65/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 74s 79ms/step - loss: 0.4045 - accuracy: 0.9360 - val_loss: 0.3812 - val_accuracy: 0.9372\n",
      "Epoch 66/120\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.4028 - accuracy: 0.9362 - val_loss: 0.3683 - val_accuracy: 0.9379\n",
      "Epoch 67/120\n",
      "938/938 [==============================] - 75s 79ms/step - loss: 0.3705 - accuracy: 0.9394 - val_loss: 0.3718 - val_accuracy: 0.9377\n",
      "Epoch 68/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4031 - accuracy: 0.9364 - val_loss: 0.3734 - val_accuracy: 0.9392\n",
      "Epoch 69/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3910 - accuracy: 0.9369 - val_loss: 0.3693 - val_accuracy: 0.9418\n",
      "Epoch 70/120\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.3923 - accuracy: 0.9390 - val_loss: 0.3725 - val_accuracy: 0.9411\n",
      "Epoch 71/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3867 - accuracy: 0.9387 - val_loss: 0.3830 - val_accuracy: 0.9393\n",
      "Epoch 72/120\n",
      "938/938 [==============================] - 78s 83ms/step - loss: 0.4025 - accuracy: 0.9377 - val_loss: 0.3801 - val_accuracy: 0.9386\n",
      "Epoch 73/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3941 - accuracy: 0.9386 - val_loss: 0.3776 - val_accuracy: 0.9388\n",
      "Epoch 74/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3834 - accuracy: 0.9384 - val_loss: 0.3529 - val_accuracy: 0.9432\n",
      "Epoch 75/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3880 - accuracy: 0.9377 - val_loss: 0.3623 - val_accuracy: 0.9445\n",
      "Epoch 76/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.4005 - accuracy: 0.9378 - val_loss: 0.3604 - val_accuracy: 0.9437\n",
      "Epoch 77/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3979 - accuracy: 0.9381 - val_loss: 0.3644 - val_accuracy: 0.9441\n",
      "Epoch 78/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3889 - accuracy: 0.9398 - val_loss: 0.3606 - val_accuracy: 0.9440\n",
      "Epoch 79/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3794 - accuracy: 0.9418 - val_loss: 0.3599 - val_accuracy: 0.9437\n",
      "Epoch 80/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3874 - accuracy: 0.9396 - val_loss: 0.3691 - val_accuracy: 0.9439\n",
      "Epoch 81/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.3797 - accuracy: 0.9405 - val_loss: 0.3615 - val_accuracy: 0.9446\n",
      "Epoch 82/120\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.3731 - accuracy: 0.9427 - val_loss: 0.3609 - val_accuracy: 0.9435\n",
      "Epoch 83/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3771 - accuracy: 0.9418 - val_loss: 0.3517 - val_accuracy: 0.9456\n",
      "Epoch 84/120\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.3761 - accuracy: 0.9413 - val_loss: 0.3526 - val_accuracy: 0.9461\n",
      "Epoch 85/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3909 - accuracy: 0.9407 - val_loss: 0.3531 - val_accuracy: 0.9472\n",
      "Epoch 86/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3820 - accuracy: 0.9421 - val_loss: 0.3555 - val_accuracy: 0.9465\n",
      "Epoch 87/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3919 - accuracy: 0.9413 - val_loss: 0.3508 - val_accuracy: 0.9470\n",
      "Epoch 88/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3830 - accuracy: 0.9422 - val_loss: 0.3430 - val_accuracy: 0.9464\n",
      "Epoch 89/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3634 - accuracy: 0.9436 - val_loss: 0.3449 - val_accuracy: 0.9461\n",
      "Epoch 90/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.3684 - accuracy: 0.9427 - val_loss: 0.3423 - val_accuracy: 0.9456\n",
      "Epoch 91/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3659 - accuracy: 0.9428 - val_loss: 0.3415 - val_accuracy: 0.9461\n",
      "Epoch 92/120\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.3614 - accuracy: 0.9438 - val_loss: 0.3435 - val_accuracy: 0.9472\n",
      "Epoch 93/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3584 - accuracy: 0.9435 - val_loss: 0.3389 - val_accuracy: 0.9468\n",
      "Epoch 94/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3695 - accuracy: 0.9431 - val_loss: 0.3361 - val_accuracy: 0.9476\n",
      "Epoch 95/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3646 - accuracy: 0.9438 - val_loss: 0.3433 - val_accuracy: 0.9474\n",
      "Epoch 96/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3742 - accuracy: 0.9431 - val_loss: 0.3450 - val_accuracy: 0.9490\n",
      "Epoch 97/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3795 - accuracy: 0.9445 - val_loss: 0.3423 - val_accuracy: 0.9478\n",
      "Epoch 98/120\n",
      "938/938 [==============================] - 76s 82ms/step - loss: 0.3658 - accuracy: 0.9439 - val_loss: 0.3408 - val_accuracy: 0.9477\n",
      "Epoch 99/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3673 - accuracy: 0.9430 - val_loss: 0.3350 - val_accuracy: 0.9471\n",
      "Epoch 100/120\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.3732 - accuracy: 0.9430 - val_loss: 0.3359 - val_accuracy: 0.9457\n",
      "Epoch 101/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3764 - accuracy: 0.9427 - val_loss: 0.3353 - val_accuracy: 0.9473\n",
      "Epoch 102/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3679 - accuracy: 0.9443 - val_loss: 0.3343 - val_accuracy: 0.9469\n",
      "Epoch 103/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3559 - accuracy: 0.9438 - val_loss: 0.3347 - val_accuracy: 0.9475\n",
      "Epoch 104/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3609 - accuracy: 0.9441 - val_loss: 0.3344 - val_accuracy: 0.9475\n",
      "Epoch 105/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3540 - accuracy: 0.9451 - val_loss: 0.3323 - val_accuracy: 0.9478\n",
      "Epoch 106/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3485 - accuracy: 0.9460 - val_loss: 0.3308 - val_accuracy: 0.9478\n",
      "Epoch 107/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.3453 - accuracy: 0.9454 - val_loss: 0.3249 - val_accuracy: 0.9483\n",
      "Epoch 108/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3548 - accuracy: 0.9455 - val_loss: 0.3238 - val_accuracy: 0.9480\n",
      "Epoch 109/120\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.3413 - accuracy: 0.9456 - val_loss: 0.3244 - val_accuracy: 0.9481\n",
      "Epoch 110/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3574 - accuracy: 0.9438 - val_loss: 0.3287 - val_accuracy: 0.9478\n",
      "Epoch 111/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3436 - accuracy: 0.9456 - val_loss: 0.3290 - val_accuracy: 0.9480\n",
      "Epoch 112/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3509 - accuracy: 0.9451 - val_loss: 0.3288 - val_accuracy: 0.9469\n",
      "Epoch 113/120\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.3560 - accuracy: 0.9455 - val_loss: 0.3294 - val_accuracy: 0.9475\n",
      "Epoch 114/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3483 - accuracy: 0.9451 - val_loss: 0.3268 - val_accuracy: 0.9494\n",
      "Epoch 115/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3485 - accuracy: 0.9465 - val_loss: 0.3281 - val_accuracy: 0.9492\n",
      "Epoch 116/120\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.3582 - accuracy: 0.9459 - val_loss: 0.3235 - val_accuracy: 0.9494\n",
      "Epoch 117/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3418 - accuracy: 0.9461 - val_loss: 0.3217 - val_accuracy: 0.9498\n",
      "Epoch 118/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3418 - accuracy: 0.9471 - val_loss: 0.3265 - val_accuracy: 0.9490\n",
      "Epoch 119/120\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.3408 - accuracy: 0.9472 - val_loss: 0.3271 - val_accuracy: 0.9501\n",
      "Epoch 120/120\n",
      "938/938 [==============================] - 69s 73ms/step - loss: 0.3344 - accuracy: 0.9471 - val_loss: 0.3249 - val_accuracy: 0.9498\n",
      "Epoch 1/120\n",
      "compute the gradient\n",
      "[False, False, False, False, False, False, False, False]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "compute the gradient\n",
      "[False, False, False, False, False, False, False, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "938/938 [==============================] - 49s 49ms/step - loss: 1.4831 - accuracy: 0.5059 - val_loss: 0.2177 - val_accuracy: 0.9335\n",
      "Epoch 2/120\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.2064 - accuracy: 0.9375 - val_loss: 0.1321 - val_accuracy: 0.9599\n",
      "Epoch 3/120\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.1356 - accuracy: 0.9589 - val_loss: 0.0950 - val_accuracy: 0.9710\n",
      "Epoch 4/120\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.1043 - accuracy: 0.9679 - val_loss: 0.0839 - val_accuracy: 0.9739\n",
      "Epoch 5/120\n",
      "938/938 [==============================] - 45s 48ms/step - loss: 0.0895 - accuracy: 0.9725 - val_loss: 0.0773 - val_accuracy: 0.9757\n",
      "Epoch 6/120\n",
      "938/938 [==============================] - 46s 49ms/step - loss: 0.0798 - accuracy: 0.9755 - val_loss: 0.0701 - val_accuracy: 0.9774\n",
      "Epoch 7/120\n",
      "938/938 [==============================] - 46s 49ms/step - loss: 0.0702 - accuracy: 0.9784 - val_loss: 0.0620 - val_accuracy: 0.9799\n",
      "Epoch 8/120\n",
      "938/938 [==============================] - 47s 50ms/step - loss: 0.0645 - accuracy: 0.9810 - val_loss: 0.0568 - val_accuracy: 0.9824\n",
      "Epoch 9/120\n",
      "938/938 [==============================] - 46s 49ms/step - loss: 0.0586 - accuracy: 0.9829 - val_loss: 0.0539 - val_accuracy: 0.9814\n",
      "Epoch 10/120\n",
      "938/938 [==============================] - 45s 48ms/step - loss: 0.0580 - accuracy: 0.9816 - val_loss: 0.0577 - val_accuracy: 0.9820\n",
      "Epoch 11/120\n",
      "938/938 [==============================] - 45s 48ms/step - loss: 0.0557 - accuracy: 0.9832 - val_loss: 0.0506 - val_accuracy: 0.9840\n",
      "Epoch 12/120\n",
      "938/938 [==============================] - 46s 49ms/step - loss: 0.0511 - accuracy: 0.9837 - val_loss: 0.0529 - val_accuracy: 0.9832\n",
      "Epoch 13/120\n",
      "938/938 [==============================] - 48s 51ms/step - loss: 0.0478 - accuracy: 0.9851 - val_loss: 0.0436 - val_accuracy: 0.9858\n",
      "Epoch 14/120\n",
      "938/938 [==============================] - 49s 52ms/step - loss: 0.0452 - accuracy: 0.9863 - val_loss: 0.0489 - val_accuracy: 0.9836\n",
      "Epoch 15/120\n",
      "938/938 [==============================] - 46s 49ms/step - loss: 0.0408 - accuracy: 0.9874 - val_loss: 0.0430 - val_accuracy: 0.9853\n",
      "Epoch 16/120\n",
      "938/938 [==============================] - 45s 48ms/step - loss: 0.0402 - accuracy: 0.9876 - val_loss: 0.0420 - val_accuracy: 0.9860\n",
      "Epoch 17/120\n",
      "139/938 [===>..........................] - ETA: 41s - loss: 0.0398 - accuracy: 0.9890"
     ]
    }
   ],
   "source": [
    "# check the training accuracy\n",
    "dpsgd = True\n",
    "parameter_list = {}\n",
    "accuracies = {}\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\", \"ALLnoise+mnist+tied_bias\", \"mnist+tied_bias+noise_input\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+untied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=lr_schedule)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule_no_dp)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_noise(x, clip, noise_multiplier):\n",
    "    def clip_features(x):\n",
    "        return tf.clip_by_global_norm([x], clip)[0][0]\n",
    "    clipped_features = tf.map_fn(clip_features, x)\n",
    "    # Add noise to summed gradients.\n",
    "    noise_stddev = clip * noise_multiplier\n",
    "    noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "    return clipped_features + noise\n",
    "\n",
    "noisy_train_data = tf.map_fn(lambda x: clip_noise(x, l2_norm_clip, noise_multiplier), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training accuracy\n",
    "\n",
    "parameter_list = {}\n",
    "model_type = \"noisy_permanent_input\"\n",
    "dpsgd = False\n",
    "file_name = model_type + \"_accuracy\"\n",
    "model = build_models(\"mnist+untied_bias\")\n",
    "if dpsgd:\n",
    "    optimizer = FixedDPKerasSGDOptimizer(\n",
    "        batch_size = batch_size,\n",
    "        num_parameters = num_parameters,\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        var_list = model.trainable_variables,\n",
    "        #num_microbatches=batch_size//microbatch_size,\n",
    "        microbatch_size = microbatch_size,\n",
    "        learning_rate=learning_rate)\n",
    "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with Keras\n",
    "checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=checkpoint_filepath,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_accuracy',\n",
    "  mode='max',\n",
    "  save_best_only=True)\n",
    "#early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "#    mode='auto', baseline=None, restore_best_weights=True\n",
    "#)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "# Train model with Keras\n",
    "if dpsgd:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=microbatch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "else:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=batch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "\n",
    "#evaluated_result = model.evaluate(\n",
    "#    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "#    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "#    return_dict=True)\n",
    "if model_type not in accuracies.keys():\n",
    "    accuracies[model_type] = []\n",
    "accuracies[model_type].append(history)\n",
    "#accuracies[model_type].append(evaluated_result[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies = dict()\n",
    "for key in accuracies.keys():   \n",
    "    processed_accuracies[key] = accuracies[key][0].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['val_accuracy'])\n",
    "#plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"dp_data/results/accuracies_%.1f_%.1f_%.1f_%d\"%(learning_rate, noise_multiplier, l2_norm_clip, batch_size),\"wb\")\n",
    "pickle.dump(processed_accuracies, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the step-epsilon curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "try:\n",
    "    with open(\"dp_data/noise_epsilon_step\") as f:\n",
    "        epsilons = pickle.load(f)\n",
    "except:\n",
    "    training_data_size = 60000\n",
    "    max_step = 25000\n",
    "    epsilons = [[] for i in range(10)]\n",
    "    for noise_multiplier in range(0, 10, 1):\n",
    "        print(\"noise_multiplier:\", noise_multiplier)\n",
    "        for step in range(1, max_step, 10):\n",
    "            epsilons[noise_multiplier].append(compute_epsilon(step, batch_size/training_data_size, noise_multiplier))\n",
    "    file = open(\"dp_data/noise_epsilon_step_%d\"%batch_size,\"wb\")\n",
    "    pickle.dump(epsilons, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tied Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tied bias\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\", \"ALLnoise+mnist+tied_bias\",\"mnist+tied_bias\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+tied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=learning_rate)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "for key in parameter_list.keys():\n",
    "    line1, = ax.plot(parameter_list[key],accuracies[key],  label=key)\n",
    "    line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee\n",
    "import pickle\n",
    "result = open(\"result\",\"wb\")\n",
    "pickle.dump(accuracies, result)\n",
    "pickle.dump(parameter_list, result)\n",
    "result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(\"result\", \"rb\")\n",
    "accuracies = pickle.load(result)\n",
    "parameter_list = pickle.load(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "line1, = ax.plot(parameter_list[\"linear\"],accuracies[\"linear\"],  label='Linear')\n",
    "line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "line2, = ax.plot(parameter_list[\"bias\"],  accuracies[\"bias\"], label='Bias')\n",
    "line2.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "a = [tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32), tf.constant([3., 4, 5], shape=(3,), dtype=tf.float32), tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32)]\n",
    "b = tf.ones_like(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linalg.global_norm(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_global_norm(a, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
