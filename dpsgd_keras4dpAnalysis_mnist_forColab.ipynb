{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "egyptian-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Training a CNN on MNIST with Keras and the DP SGD optimizer.\n",
    "Slow implementation allowing large batch size: using input-size=num_microbatch outside, B-batch inside.\n",
    "Usage: input batch size also, which is the real update frequency.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(3)\n",
    "\n",
    "def random_choice_cond(x, size):\n",
    "    tensor_size = tf.size(x)\n",
    "    indices = tf.range(0, tensor_size, dtype=tf.int64)\n",
    "    if size == 0:\n",
    "        sample_flatten_index = tf.random.shuffle(indices)[:]\n",
    "    else:     \n",
    "        sample_flatten_index = tf.random.shuffle(indices)[:size]\n",
    "    sample_index = tf.transpose(tf.unravel_index(tf.cast(sample_flatten_index,tf.int32), tf.shape(input=x))) #[Result: [indexes for the first sample], [indexes for the second sample]...]\n",
    "    cond = tf.scatter_nd(sample_index, tf.ones(tf.shape(input=sample_index)[0],dtype=tf.bool), tf.shape(input=x))\n",
    "    return cond\n",
    "\n",
    "    # we need noise on the same fixed number of model parameters for each layer.\n",
    "\n",
    "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "def make_fixed_keras_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP Keras optimizer class from an existing one.\"\"\"\n",
    "\n",
    "  class FixedDPOptimizerClass(cls):\n",
    "    \"\"\"Differentially private subclass of given class cls.\n",
    "    The class tf.keras.optimizers.Optimizer has two methods to compute\n",
    "    gradients, `_compute_gradients` and `get_gradients`. The first works\n",
    "    with eager execution, while the second runs in graph mode and is used\n",
    "    by canned estimators.\n",
    "    Internally, DPOptimizerClass stores hyperparameters both individually\n",
    "    and encapsulated in a `GaussianSumQuery` object for these two use cases.\n",
    "    However, this should be invisible to users of this class.\n",
    "    \n",
    "    \n",
    "    btw. support negative num_parameters, used for all but num_parameters noises.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2_norm_clip,\n",
    "        noise_multiplier,\n",
    "        batch_size,\n",
    "        var_list,\n",
    "        microbatch_size= 1,\n",
    "        num_parameters = 10,\n",
    "        #noise_layer_type = \"linear\",\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
    "        **kwargs):\n",
    "      \"\"\"Initialize the DPOptimizerClass.\n",
    "      Args:\n",
    "        l2_norm_clip: Clipping norm (max L2 norm of per microbatch gradients)\n",
    "        noise_multiplier: Ratio of the standard deviation to the clipping norm\n",
    "        num_microbatches: The number of microbatches into which each minibatch\n",
    "          is split.\n",
    "      \"\"\"\n",
    "      super(FixedDPOptimizerClass, self).__init__(*args, **kwargs)\n",
    "      assert(batch_size % microbatch_size == 0)\n",
    "      self._l2_norm_clip = l2_norm_clip\n",
    "      self._noise_multiplier = noise_multiplier\n",
    "      self._aggregate_gradients = [tf.Variable(tf.zeros_like(grad)) for grad in var_list]\n",
    "      self._dp_sum_query = gaussian_query.GaussianSumQuery(\n",
    "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
    "      self._global_state = None\n",
    "      self._num_parameters = num_parameters\n",
    "      self._was_dp_gradients_called = False\n",
    "      self._batch_idx = tf.Variable(0)\n",
    "      self._batch_size = tf.Variable(batch_size)\n",
    "      self._microbatch_size = microbatch_size\n",
    "      self.samples_cond = {}\n",
    "\n",
    "    def _compute_gradients(self, loss, var_list, grad_loss=None, tape=None):\n",
    "      \"\"\"DP version of superclass method.\"\"\"\n",
    "      print(\"compute the gradient\")\n",
    "      is_considered = [x.name.startswith(\"Considered\") for x in var_list]\n",
    "    \n",
    "      print(is_considered)\n",
    "      self._was_dp_gradients_called = True\n",
    "      # Precompute the noise locations\n",
    "      if len(self.samples_cond) == 0:\n",
    "        self.samples_cond = [tf.Variable(random_choice_cond(x, self._num_parameters)) for x in var_list]\n",
    "      # Compute loss.\n",
    "      if not callable(loss) and tape is None:\n",
    "        raise ValueError('`tape` is required when a `Tensor` loss is passed.')\n",
    "      tape = tape if tape is not None else tf.GradientTape()\n",
    "      self.is_considered = is_considered\n",
    "      if callable(loss):\n",
    "        with tape:\n",
    "          if not callable(var_list):\n",
    "            tape.watch(var_list)\n",
    "          if callable(loss):\n",
    "            loss = loss()\n",
    "          if callable(var_list):\n",
    "            var_list = var_list()\n",
    "      var_list = tf.nest.flatten(var_list)\n",
    "\n",
    "      # Compute the per-microbatch losses using helpful jacobian method.\n",
    "      with tf.keras.backend.name_scope(self._name + '/gradients'):\n",
    "        #tf.print(microbatch_losses.shape)\n",
    "        jacobian = tape.jacobian(loss, var_list)\n",
    "        # print(jacobian)\n",
    "        # Clip gradients to given l2_norm_clip.\n",
    "        def clip_gradients(g):\n",
    "          #tf.print(\"g:\", g)\n",
    "          #tf.print(tf.linalg.global_norm(g))\n",
    "          considered_g = [g[i] for i in range(len(g)) if self.is_considered[i]]\n",
    "          div_scale = tf.linalg.global_norm(considered_g)/self._l2_norm_clip\n",
    "          if div_scale > 1:\n",
    "            return [grad/div_scale for grad in g]\n",
    "          else:\n",
    "            return g\n",
    "\n",
    "        clipped_gradients = tf.map_fn(clip_gradients, jacobian)\n",
    "        print(\"clipped_gradients:\", clipped_gradients)\n",
    "        \n",
    "        final_gradients = [tf.reduce_sum(clipped_gradients[i], axis=0) for i in range(len(clipped_gradients))]\n",
    "        print(final_gradients)\n",
    "        \n",
    "        #self.clipped_gradients = clipped_gradients\n",
    "        #self.final_gradients = final_gradients\n",
    "        _aggregate_gradients = self._aggregate_gradients.copy()\n",
    "        #tf.print(_aggregate_gradients)\n",
    "        _batch_idx = self._batch_idx\n",
    "        _batch_size = self._batch_size\n",
    "        \n",
    "        for i in range(len(final_gradients)):\n",
    "          _aggregate_gradients[i] = tf.cond(_batch_idx == tf.constant(0), lambda :final_gradients[i], lambda :_aggregate_gradients[i] + final_gradients[i])\n",
    "        \n",
    "        for i in range(len(_aggregate_gradients)):\n",
    "          self._aggregate_gradients[i].assign(_aggregate_gradients[i])\n",
    "        \n",
    "        _batch_idx = self._microbatch_size + _batch_idx \n",
    "        #tf.print(_batch_idx)\n",
    "        def noise_normalize_batch(self, g, is_considered, layer_index):\n",
    "          # Sample the indexes\n",
    "          sampled_cond = self.samples_cond[layer_index]\n",
    "          # Add noise to summed gradients.\n",
    "          noise_stddev = self._l2_norm_clip * self._noise_multiplier\n",
    "          noise = tf.random.normal(\n",
    "            tf.shape(input=g), stddev=noise_stddev)\n",
    "          noised_gradient = tf.add(g, noise)\n",
    "          fixed_gradient = tf.where(tf.math.logical_and(sampled_cond, is_considered), noised_gradient, g)\n",
    "          # Normalize by number of microbatches and return.\n",
    "          return tf.truediv(fixed_gradient, tf.cast(_batch_size, dtype=tf.float32))\n",
    "        \n",
    "        noise_normalized_gradients = [noise_normalize_batch(self, _aggregate_gradients[i], self.is_considered[i], i) for i in range(len(_aggregate_gradients))]\n",
    "        for i in range(len(final_gradients)):\n",
    "          final_gradients[i] = tf.cond(_batch_idx >= _batch_size, lambda :noise_normalized_gradients[i], lambda :tf.zeros_like(final_gradients[i]))\n",
    "        _batch_idx = tf.cond(_batch_idx >= _batch_size, lambda : tf.constant(0), lambda :_batch_idx + 0)\n",
    "      #tf.print(final_gradients)\n",
    "      self._batch_idx.assign(_batch_idx)\n",
    "      return list(zip(final_gradients, var_list))\n",
    "\n",
    "    #_aggregate_gradients: aggregated gradients until now.\n",
    "    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "      assert self._was_dp_gradients_called, (\n",
    "          'Neither _compute_gradients() or get_gradients() on the '\n",
    "          'differentially private optimizer was called. This means the '\n",
    "          'training is not differentially private. It may be the case that '\n",
    "          'you need to upgrade to TF 2.4 or higher to use this particular '\n",
    "          'optimizer.')\n",
    "      return super(FixedDPOptimizerClass,\n",
    "                   self).apply_gradients(grads_and_vars, global_step, name)\n",
    "  return FixedDPOptimizerClass\n",
    "\n",
    "FixedDPKerasSGDOptimizer = make_fixed_keras_optimizer_class(tf.keras.optimizers.SGD)\n",
    "\n",
    "def compute_epsilon(steps, sampling_probability, noise_multiplier):\n",
    "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "  if noise_multiplier == 0.0:\n",
    "    return float('inf')\n",
    "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "  rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=noise_multiplier,\n",
    "                    steps=steps,\n",
    "                    orders=orders)\n",
    "  # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
    "\n",
    "class BiasLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(BiasLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight('bias',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "    def call(self, x):\n",
    "        return x + self.bias\n",
    "    \n",
    "    \n",
    "class TiedBiasLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TiedBiasLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight('bias',\n",
    "                                    shape=input_shape[-1:],\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "    def call(self, x):\n",
    "        return x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eligible-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: 2021/03/14\n",
    "# Usage: Dp analysis of mnist neural network.\n",
    "# Detail: Check the notion notes in HE-DP project.\n",
    "\n",
    "dpsgd = False # add dp noise or not \n",
    "learning_rate = 0.1\n",
    "noise_multiplier = 4\n",
    "l2_norm_clip = 4\n",
    "batch_size = 512\n",
    "epochs = 120\n",
    "microbatch_size = 64\n",
    "num_parameters = 0\n",
    "privacy_budget = []\n",
    "delta = 1e-5  # it is recommended to use delta~=1/dataset_size\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "architectural-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n",
    "  test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nasty-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturnbing the input dataset, and collect the accuracy-step\n",
    "class GaussianNoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, l2_norm_clip, noise_multiplier, *args, **kwargs):\n",
    "        super(GaussianNoiseLayer, self).__init__(*args, **kwargs)\n",
    "        self._l2_norm_clip = l2_norm_clip\n",
    "        self._noise_multiplier = noise_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Clip gradients to given l2_norm_clip.\n",
    "        def clip_features(x):\n",
    "            return tf.clip_by_global_norm([x], self._l2_norm_clip)[0][0]\n",
    "\n",
    "        clipped_features = tf.map_fn(clip_features, x)\n",
    "        \n",
    "        # Add noise to summed gradients.\n",
    "        noise_stddev = self._l2_norm_clip * self._noise_multiplier\n",
    "        noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "        return clipped_features + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "south-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "def build_models(noise_layer_name):\n",
    "    if noise_layer_name ==\"mnist+untied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])        \n",
    "    elif noise_layer_name == \"mnist+tied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])   \n",
    "    elif noise_layer_name == \"mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"mnist\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])  \n",
    "    elif noise_layer_name == \"sphinx+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"sphinx+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(name=\"Considered2\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            BiasLayer(name=\"Considered8\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+mnist+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(name=\"Considered7\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(16, 5,\n",
    "                                 strides=1,\n",
    "                                 padding='valid',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(28, 28, 1), use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(name=\"Considered8\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered4\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered6\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10\":\n",
    "        pass\n",
    "    else:\n",
    "        model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brave-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main_simple(unused_argv):\n",
    "logging.set_verbosity(logging.INFO)\n",
    "if dpsgd and batch_size % microbatch_size != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // microbatch_size // 1000,\n",
    "    decay_rate=0.998)\n",
    "\n",
    "\n",
    "lr_schedule_no_dp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // batch_size // 1000,\n",
    "    decay_rate=0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(None, 5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(None, 5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(None, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(None, 256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(None, 100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(None, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(5, 5, 1, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(5, 5, 16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(256, 100) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(100,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(100, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(10,) dtype=float32>]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/Considered1/Conv2D (defined at <ipython-input-7-75e03114ae70>:49) ]]\n\t [[SGD/gradients/GreaterEqual_3/_92]]\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/Considered1/Conv2D (defined at <ipython-input-7-75e03114ae70>:49) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_4070]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-75e03114ae70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicrobatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         history = model.fit(train_data, train_labels,\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/dpsgd/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/Considered1/Conv2D (defined at <ipython-input-7-75e03114ae70>:49) ]]\n\t [[SGD/gradients/GreaterEqual_3/_92]]\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/Considered1/Conv2D (defined at <ipython-input-7-75e03114ae70>:49) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_4070]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# check the training accuracy\n",
    "dpsgd = True\n",
    "parameter_list = {}\n",
    "accuracies = {}\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\", \"ALLnoise+mnist+tied_bias\", \"mnist+tied_bias+noise_input\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+untied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=lr_schedule)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule_no_dp)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbed label\n",
    "def clip_noise(x, clip, noise_multiplier):\n",
    "    def clip_features(x):\n",
    "        return tf.clip_by_global_norm([x], clip)[0][0]\n",
    "    clipped_features = tf.map_fn(clip_features, x)\n",
    "    # Add noise to summed gradients.\n",
    "    noise_stddev = clip * noise_multiplier\n",
    "    noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "    return clipped_features + noise\n",
    "\n",
    "noisy_train_data = tf.map_fn(lambda x: clip_noise(x, l2_norm_clip, noise_multiplier), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training accuracy\n",
    "\n",
    "parameter_list = {}\n",
    "model_type = \"noisy_permanent_input\"\n",
    "dpsgd = False\n",
    "file_name = model_type + \"_accuracy\"\n",
    "model = build_models(\"mnist+untied_bias\")\n",
    "if dpsgd:\n",
    "    optimizer = FixedDPKerasSGDOptimizer(\n",
    "        batch_size = batch_size,\n",
    "        num_parameters = num_parameters,\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        var_list = model.trainable_variables,\n",
    "        #num_microbatches=batch_size//microbatch_size,\n",
    "        microbatch_size = microbatch_size,\n",
    "        learning_rate=learning_rate)\n",
    "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with Keras\n",
    "checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=checkpoint_filepath,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_accuracy',\n",
    "  mode='max',\n",
    "  save_best_only=True)\n",
    "#early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "#    mode='auto', baseline=None, restore_best_weights=True\n",
    "#)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "# Train model with Keras\n",
    "if dpsgd:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=microbatch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "else:\n",
    "    history = model.fit(noisy_train_data, train_labels,\n",
    "            epochs=epochs,\n",
    "            validation_data=(test_data, test_labels),\n",
    "            batch_size=batch_size, \n",
    "            callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "\n",
    "#evaluated_result = model.evaluate(\n",
    "#    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "#    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "#    return_dict=True)\n",
    "if model_type not in accuracies.keys():\n",
    "    accuracies[model_type] = []\n",
    "accuracies[model_type].append(history)\n",
    "#accuracies[model_type].append(evaluated_result[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies = dict()\n",
    "for key in accuracies.keys():   \n",
    "    processed_accuracies[key] = accuracies[key][0].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['sphinx+mnist+tied_bias']['val_accuracy'])\n",
    "#plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['ALLnoise+mnist+tied_bias']['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"dp_data/results/accuracies_%.1f_%.1f_%.1f_%d\"%(learning_rate, noise_multiplier, l2_norm_clip, batch_size),\"wb\")\n",
    "pickle.dump(processed_accuracies, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
