{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: 2021/03/14\n",
    "# Usage: Dp analysis of mnist neural network.\n",
    "# Detail: Check the notion notes in HE-DP project.\n",
    "\n",
    "from dpsgd_keras_slow import *\n",
    "\n",
    "dpsgd = False # add dp noise or not \n",
    "learning_rate = 0.1\n",
    "noise_multiplier = 1\n",
    "l2_norm_clip = 3\n",
    "batch_size = 1024\n",
    "epochs = 40\n",
    "microbatch_size = 16\n",
    "num_parameters = 0\n",
    "privacy_budget = []\n",
    "delta = 1e-5  # it is recommended to use delta~=1/dataset_size\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7,8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.cifar10.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_data = train_data.reshape((train_data.shape[0], 32, 32, 3))\n",
    "  test_data = test_data.reshape((test_data.shape[0], 32, 32, 3))\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturnbing the input dataset, and collect the accuracy-step\n",
    "class GaussianNoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, l2_norm_clip, noise_multiplier, *args, **kwargs):\n",
    "        super(GaussianNoiseLayer, self).__init__(*args, **kwargs)\n",
    "        self._l2_norm_clip = l2_norm_clip\n",
    "        self._noise_multiplier = noise_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Clip gradients to given l2_norm_clip.\n",
    "        def clip_features(x):\n",
    "            return tf.clip_by_global_norm([x], self._l2_norm_clip)[0][0]\n",
    "\n",
    "        clipped_features = tf.map_fn(clip_features, x)\n",
    "        \n",
    "        # Add noise to summed gradients.\n",
    "        noise_stddev = self._l2_norm_clip * self._noise_multiplier\n",
    "        noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "        return clipped_features + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.clip_by_global_norm([tf.Variable([[2,3],[4,5]],shape=[2,2],dtype=tf.float32)], 4.4)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "def build_models(noise_layer_name):\n",
    "    if noise_layer_name ==\"cifar10+untied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10+tied_bias+noise_input\":\n",
    "        model = tf.keras.Sequential([\n",
    "            GaussianNoiseLayer(l2_norm_clip, noise_multiplier, input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"cifar10\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False),\n",
    "            BiasLayer()\n",
    "        ])  \n",
    "    elif noise_layer_name == \"sphinx+cifar10+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered2\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered6\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            BiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered8\"),\n",
    "            BiasLayer()\n",
    "        ])\n",
    "    elif noise_layer_name == \"sphinx+cifar10+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered6\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            TiedBiasLayer(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered8\"),\n",
    "            BiasLayer()\n",
    "        ])        \n",
    "    elif noise_layer_name == \"ALLnoise+cifar10+untied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False, name=\"Considered1\"),\n",
    "            BiasLayer(name=\"Considered9\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered2\"),\n",
    "            BiasLayer(name=\"Considered10\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            BiasLayer(name=\"Considered11\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            BiasLayer(name=\"Considered12\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            BiasLayer(name=\"Considered13\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered6\"),\n",
    "            BiasLayer(name=\"Considered14\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            BiasLayer(name=\"Considered15\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered8\"),\n",
    "            BiasLayer(name=\"Considered16\")\n",
    "        ])\n",
    "    elif noise_layer_name == \"ALLnoise+cifar10+tied_bias\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None,\n",
    "                                 input_shape=(32, 32, 3), use_bias=False, name=\"Considered1\"),\n",
    "            TiedBiasLayer(name=\"Considered9\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered2\"),\n",
    "            TiedBiasLayer(name=\"Considered10\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered3\"),\n",
    "            TiedBiasLayer(name=\"Considered11\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered4\"),\n",
    "            TiedBiasLayer(name=\"Considered12\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2,2), 2),\n",
    "            tf.keras.layers.Conv2D(64, 3,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered5\"),\n",
    "            TiedBiasLayer(name=\"Considered13\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered6\"),\n",
    "            TiedBiasLayer(name=\"Considered14\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(16, 1,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation=None, use_bias=False, name=\"Considered7\"),\n",
    "            TiedBiasLayer(name=\"Considered15\"),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=None, use_bias=False, name=\"Considered8\"),\n",
    "            BiasLayer(name=\"Considered16\")\n",
    "        ])\n",
    "    else:\n",
    "        model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_layer (Gaussi (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1728      \n",
      "_________________________________________________________________\n",
      "bias_layer (BiasLayer)       (None, 32, 32, 64)        65536     \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_1 (BiasLayer)     (None, 32, 32, 64)        65536     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_2 (BiasLayer)     (None, 16, 16, 64)        16384     \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_3 (BiasLayer)     (None, 16, 16, 64)        16384     \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_4 (BiasLayer)     (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "bias_layer_5 (BiasLayer)     (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "bias_layer_6 (BiasLayer)     (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                10240     \n",
      "_________________________________________________________________\n",
      "bias_layer_7 (BiasLayer)     (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 337,610\n",
      "Trainable params: 337,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"cifar10+untied_bias+noise_input\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 64)        1728      \n",
      "_________________________________________________________________\n",
      "bias_layer_8 (BiasLayer)     (None, 32, 32, 64)        65536     \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 64)        36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_9 (BiasLayer)     (None, 32, 32, 64)        65536     \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_10 (BiasLayer)    (None, 16, 16, 64)        16384     \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_11 (BiasLayer)    (None, 16, 16, 64)        16384     \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 64)          36864     \n",
      "_________________________________________________________________\n",
      "bias_layer_12 (BiasLayer)    (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "bias_layer_13 (BiasLayer)    (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "bias_layer_14 (BiasLayer)    (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10240     \n",
      "_________________________________________________________________\n",
      "bias_layer_15 (BiasLayer)    (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 337,610\n",
      "Trainable params: 337,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"cifar10+untied_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 64)        1728      \n",
      "_________________________________________________________________\n",
      "tied_bias_layer (TiedBiasLay (None, 32, 32, 64)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 64)        36864     \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_1 (TiedBiasL (None, 32, 32, 64)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_2 (TiedBiasL (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_16 (ReLU)              (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_3 (TiedBiasL (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_17 (ReLU)              (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 64)          36864     \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_4 (TiedBiasL (None, 8, 8, 64)          64        \n",
      "_________________________________________________________________\n",
      "re_lu_18 (ReLU)              (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_5 (TiedBiasL (None, 8, 8, 64)          64        \n",
      "_________________________________________________________________\n",
      "re_lu_19 (ReLU)              (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "tied_bias_layer_6 (TiedBiasL (None, 8, 8, 16)          16        \n",
      "_________________________________________________________________\n",
      "re_lu_20 (ReLU)              (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10240     \n",
      "_________________________________________________________________\n",
      "bias_layer_16 (BiasLayer)    (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 164,954\n",
      "Trainable params: 164,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"cifar10+tied_bias\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 64)        1728      \n",
      "_________________________________________________________________\n",
      "re_lu_21 (ReLU)              (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 32, 64)        36864     \n",
      "_________________________________________________________________\n",
      "re_lu_22 (ReLU)              (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "re_lu_23 (ReLU)              (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 16, 16, 64)        36864     \n",
      "_________________________________________________________________\n",
      "re_lu_24 (ReLU)              (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 64)          36864     \n",
      "_________________________________________________________________\n",
      "re_lu_25 (ReLU)              (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 8, 8, 64)          4096      \n",
      "_________________________________________________________________\n",
      "re_lu_26 (ReLU)              (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 8, 8, 16)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_27 (ReLU)              (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10240     \n",
      "_________________________________________________________________\n",
      "bias_layer_17 (BiasLayer)    (None, 10)                10        \n",
      "=================================================================\n",
      "Total params: 164,554\n",
      "Trainable params: 164,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_models(\"cifar10\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main_simple(unused_argv):\n",
    "logging.set_verbosity(logging.INFO)\n",
    "if dpsgd and batch_size % microbatch_size != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=(epochs * train_data.shape[0]) // microbatch_size // 1000,\n",
    "    decay_rate=0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(16, 3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_8/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_9/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_10/TensorListStack:0' shape=(16, 1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_11/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_12/TensorListStack:0' shape=(16, 1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_13/TensorListStack:0' shape=(16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_14/TensorListStack:0' shape=(16, 1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_15/TensorListStack:0' shape=(16, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_8:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_9:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_10:0' shape=(1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_11:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_12:0' shape=(1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_13:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_14:0' shape=(1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_15:0' shape=(10,) dtype=float32>]\n",
      "compute the gradient\n",
      "[True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(16, 3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_8/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_9/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_10/TensorListStack:0' shape=(16, 1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_11/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_12/TensorListStack:0' shape=(16, 1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_13/TensorListStack:0' shape=(16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_14/TensorListStack:0' shape=(16, 1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_15/TensorListStack:0' shape=(16, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_8:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_9:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_10:0' shape=(1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_11:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_12:0' shape=(1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_13:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_14:0' shape=(1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_15:0' shape=(10,) dtype=float32>]\n",
      "3125/3125 [==============================] - 163s 31ms/step - loss: 2.2980 - accuracy: 0.1316 - val_loss: 2.2624 - val_accuracy: 0.1354\n",
      "Epoch 2/40\n",
      "3125/3125 [==============================] - 100s 32ms/step - loss: 2.2468 - accuracy: 0.1647 - val_loss: 2.2343 - val_accuracy: 0.1853\n",
      "Epoch 3/40\n",
      "3125/3125 [==============================] - 100s 32ms/step - loss: 2.1709 - accuracy: 0.2146 - val_loss: 2.0515 - val_accuracy: 0.2582\n",
      "Epoch 4/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 2.1025 - accuracy: 0.2457 - val_loss: 2.0026 - val_accuracy: 0.2889\n",
      "Epoch 5/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 2.0476 - accuracy: 0.2717 - val_loss: 2.0331 - val_accuracy: 0.2780\n",
      "Epoch 6/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 2.0442 - accuracy: 0.2829 - val_loss: 1.9551 - val_accuracy: 0.3134\n",
      "Epoch 7/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 1.9915 - accuracy: 0.3017 - val_loss: 1.9221 - val_accuracy: 0.3209\n",
      "Epoch 8/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 1.9752 - accuracy: 0.3102 - val_loss: 1.9714 - val_accuracy: 0.3198\n",
      "Epoch 9/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 1.9448 - accuracy: 0.3195 - val_loss: 1.9211 - val_accuracy: 0.3277\n",
      "Epoch 10/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 1.9331 - accuracy: 0.3236 - val_loss: 1.9473 - val_accuracy: 0.3263\n",
      "Epoch 11/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 1.9212 - accuracy: 0.3330 - val_loss: 1.8798 - val_accuracy: 0.3495\n",
      "Epoch 12/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 1.9125 - accuracy: 0.3354 - val_loss: 1.8926 - val_accuracy: 0.3465\n",
      "Epoch 13/40\n",
      "3125/3125 [==============================] - 101s 32ms/step - loss: 1.9074 - accuracy: 0.3416 - val_loss: 1.9166 - val_accuracy: 0.3461\n",
      "Epoch 14/40\n",
      "3125/3125 [==============================] - 109s 35ms/step - loss: 1.8912 - accuracy: 0.3488 - val_loss: 1.8799 - val_accuracy: 0.3544\n",
      "Epoch 15/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.9059 - accuracy: 0.3522 - val_loss: 1.8948 - val_accuracy: 0.3539\n",
      "Epoch 16/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8856 - accuracy: 0.3577 - val_loss: 1.8244 - val_accuracy: 0.3688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8662 - accuracy: 0.3641 - val_loss: 1.9180 - val_accuracy: 0.3576\n",
      "Epoch 18/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8670 - accuracy: 0.3663 - val_loss: 1.8456 - val_accuracy: 0.3734\n",
      "Epoch 19/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8797 - accuracy: 0.3657 - val_loss: 1.9018 - val_accuracy: 0.3637\n",
      "Epoch 20/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8790 - accuracy: 0.3704 - val_loss: 1.8077 - val_accuracy: 0.3826\n",
      "Epoch 21/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8411 - accuracy: 0.3824 - val_loss: 1.9186 - val_accuracy: 0.3626\n",
      "Epoch 22/40\n",
      "3125/3125 [==============================] - 115s 37ms/step - loss: 1.8553 - accuracy: 0.3798 - val_loss: 1.9105 - val_accuracy: 0.3763\n",
      "Epoch 23/40\n",
      "3125/3125 [==============================] - 115s 37ms/step - loss: 1.8752 - accuracy: 0.3788 - val_loss: 1.7916 - val_accuracy: 0.4006\n",
      "Epoch 24/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8606 - accuracy: 0.3859 - val_loss: 1.7968 - val_accuracy: 0.3955\n",
      "Epoch 25/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8338 - accuracy: 0.3908 - val_loss: 1.8721 - val_accuracy: 0.3889\n",
      "Epoch 26/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8552 - accuracy: 0.3910 - val_loss: 1.8214 - val_accuracy: 0.3976\n",
      "Epoch 27/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8563 - accuracy: 0.3955 - val_loss: 1.8030 - val_accuracy: 0.4042\n",
      "Epoch 28/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8365 - accuracy: 0.4030 - val_loss: 1.8717 - val_accuracy: 0.3894\n",
      "Epoch 29/40\n",
      "3125/3125 [==============================] - 115s 37ms/step - loss: 1.8467 - accuracy: 0.4030 - val_loss: 1.8388 - val_accuracy: 0.4093\n",
      "Epoch 30/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8188 - accuracy: 0.4142 - val_loss: 1.7674 - val_accuracy: 0.4247\n",
      "Epoch 31/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8469 - accuracy: 0.4131 - val_loss: 1.7803 - val_accuracy: 0.4181\n",
      "Epoch 32/40\n",
      "3125/3125 [==============================] - 115s 37ms/step - loss: 1.8285 - accuracy: 0.4141 - val_loss: 1.7729 - val_accuracy: 0.4244\n",
      "Epoch 33/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.7901 - accuracy: 0.4284 - val_loss: 1.7579 - val_accuracy: 0.4344\n",
      "Epoch 34/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8376 - accuracy: 0.4183 - val_loss: 1.7876 - val_accuracy: 0.4292\n",
      "Epoch 35/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8244 - accuracy: 0.4236 - val_loss: 1.7485 - val_accuracy: 0.4401\n",
      "Epoch 36/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8237 - accuracy: 0.4281 - val_loss: 1.9193 - val_accuracy: 0.4050\n",
      "Epoch 37/40\n",
      "3125/3125 [==============================] - 115s 37ms/step - loss: 1.8318 - accuracy: 0.4267 - val_loss: 1.7633 - val_accuracy: 0.4398\n",
      "Epoch 38/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8147 - accuracy: 0.4323 - val_loss: 1.8246 - val_accuracy: 0.4367\n",
      "Epoch 39/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.7998 - accuracy: 0.4347 - val_loss: 1.7512 - val_accuracy: 0.4440\n",
      "Epoch 40/40\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 1.8030 - accuracy: 0.4380 - val_loss: 1.8161 - val_accuracy: 0.4302\n",
      "Epoch 1/40\n",
      "compute the gradient\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(16, 3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_8/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_9/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_10/TensorListStack:0' shape=(16, 1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_11/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_12/TensorListStack:0' shape=(16, 1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_13/TensorListStack:0' shape=(16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_14/TensorListStack:0' shape=(16, 1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_15/TensorListStack:0' shape=(16, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_8:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_9:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_10:0' shape=(1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_11:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_12:0' shape=(1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_13:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_14:0' shape=(1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_15:0' shape=(10,) dtype=float32>]\n",
      "compute the gradient\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(16, 3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_8/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_9/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_10/TensorListStack:0' shape=(16, 1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_11/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_12/TensorListStack:0' shape=(16, 1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_13/TensorListStack:0' shape=(16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_14/TensorListStack:0' shape=(16, 1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_15/TensorListStack:0' shape=(16, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_8:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_9:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_10:0' shape=(1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_11:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_12:0' shape=(1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_13:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_14:0' shape=(1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_15:0' shape=(10,) dtype=float32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 122s 38ms/step - loss: 2.2929 - accuracy: 0.1433 - val_loss: 2.2451 - val_accuracy: 0.1694\n",
      "Epoch 2/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 2.2207 - accuracy: 0.1859 - val_loss: 2.1363 - val_accuracy: 0.2304\n",
      "Epoch 3/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 2.1204 - accuracy: 0.2200 - val_loss: 2.0943 - val_accuracy: 0.2360\n",
      "Epoch 4/40\n",
      "3125/3125 [==============================] - 122s 39ms/step - loss: 2.0882 - accuracy: 0.2432 - val_loss: 2.2392 - val_accuracy: 0.2504\n",
      "Epoch 5/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 2.0855 - accuracy: 0.2558 - val_loss: 2.0812 - val_accuracy: 0.2602\n",
      "Epoch 6/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 2.0484 - accuracy: 0.2737 - val_loss: 1.9957 - val_accuracy: 0.2986\n",
      "Epoch 7/40\n",
      "3125/3125 [==============================] - 120s 38ms/step - loss: 2.0372 - accuracy: 0.2854 - val_loss: 1.9689 - val_accuracy: 0.3049\n",
      "Epoch 8/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.9917 - accuracy: 0.3007 - val_loss: 2.0365 - val_accuracy: 0.2926\n",
      "Epoch 9/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 2.0002 - accuracy: 0.3000 - val_loss: 2.0140 - val_accuracy: 0.3091\n",
      "Epoch 10/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.9606 - accuracy: 0.3160 - val_loss: 1.9865 - val_accuracy: 0.3202\n",
      "Epoch 11/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.9661 - accuracy: 0.3221 - val_loss: 1.9516 - val_accuracy: 0.3276\n",
      "Epoch 12/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.9052 - accuracy: 0.3416 - val_loss: 1.9096 - val_accuracy: 0.3434\n",
      "Epoch 13/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.9192 - accuracy: 0.3373 - val_loss: 1.8677 - val_accuracy: 0.3510\n",
      "Epoch 14/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.9002 - accuracy: 0.3442 - val_loss: 1.8601 - val_accuracy: 0.3604\n",
      "Epoch 15/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8804 - accuracy: 0.3496 - val_loss: 1.8664 - val_accuracy: 0.3627\n",
      "Epoch 16/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8776 - accuracy: 0.3544 - val_loss: 1.9395 - val_accuracy: 0.3434\n",
      "Epoch 17/40\n",
      "3125/3125 [==============================] - 122s 39ms/step - loss: 1.8813 - accuracy: 0.3554 - val_loss: 1.9386 - val_accuracy: 0.3536\n",
      "Epoch 18/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8737 - accuracy: 0.3610 - val_loss: 1.8540 - val_accuracy: 0.3655\n",
      "Epoch 19/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8840 - accuracy: 0.3564 - val_loss: 1.8521 - val_accuracy: 0.3695\n",
      "Epoch 20/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8483 - accuracy: 0.3681 - val_loss: 1.8276 - val_accuracy: 0.3819\n",
      "Epoch 21/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8487 - accuracy: 0.3729 - val_loss: 1.8485 - val_accuracy: 0.3761\n",
      "Epoch 22/40\n",
      "3125/3125 [==============================] - 120s 39ms/step - loss: 1.8457 - accuracy: 0.3768 - val_loss: 1.8529 - val_accuracy: 0.3805\n",
      "Epoch 23/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8706 - accuracy: 0.3761 - val_loss: 1.8298 - val_accuracy: 0.3863\n",
      "Epoch 24/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8462 - accuracy: 0.3832 - val_loss: 1.8477 - val_accuracy: 0.3849\n",
      "Epoch 25/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8536 - accuracy: 0.3798 - val_loss: 1.8312 - val_accuracy: 0.3893\n",
      "Epoch 26/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8424 - accuracy: 0.3856 - val_loss: 1.8278 - val_accuracy: 0.3954\n",
      "Epoch 27/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8565 - accuracy: 0.3848 - val_loss: 1.8299 - val_accuracy: 0.3926\n",
      "Epoch 28/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8445 - accuracy: 0.3887 - val_loss: 1.8221 - val_accuracy: 0.4007\n",
      "Epoch 29/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8318 - accuracy: 0.3968 - val_loss: 1.7696 - val_accuracy: 0.4069\n",
      "Epoch 30/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8075 - accuracy: 0.3992 - val_loss: 1.7891 - val_accuracy: 0.4122\n",
      "Epoch 31/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8352 - accuracy: 0.3973 - val_loss: 1.8085 - val_accuracy: 0.4089\n",
      "Epoch 32/40\n",
      "3125/3125 [==============================] - 120s 39ms/step - loss: 1.8605 - accuracy: 0.3983 - val_loss: 1.8740 - val_accuracy: 0.4013\n",
      "Epoch 33/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8160 - accuracy: 0.4103 - val_loss: 1.8350 - val_accuracy: 0.4133\n",
      "Epoch 34/40\n",
      "3125/3125 [==============================] - 120s 38ms/step - loss: 1.8067 - accuracy: 0.4148 - val_loss: 1.8296 - val_accuracy: 0.4090\n",
      "Epoch 35/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8268 - accuracy: 0.4097 - val_loss: 1.8137 - val_accuracy: 0.4141\n",
      "Epoch 36/40\n",
      "3125/3125 [==============================] - 120s 39ms/step - loss: 1.8320 - accuracy: 0.4129 - val_loss: 1.7779 - val_accuracy: 0.4284\n",
      "Epoch 37/40\n",
      "3125/3125 [==============================] - 119s 38ms/step - loss: 1.8122 - accuracy: 0.4176 - val_loss: 1.8116 - val_accuracy: 0.4227\n",
      "Epoch 38/40\n",
      "3125/3125 [==============================] - 120s 38ms/step - loss: 1.8355 - accuracy: 0.4157 - val_loss: 1.7773 - val_accuracy: 0.4280\n",
      "Epoch 39/40\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 1.8410 - accuracy: 0.4144 - val_loss: 1.7713 - val_accuracy: 0.4277\n",
      "Epoch 40/40\n",
      "3125/3125 [==============================] - 120s 38ms/step - loss: 1.8087 - accuracy: 0.4250 - val_loss: 1.8209 - val_accuracy: 0.4312\n",
      "Epoch 1/40\n",
      "compute the gradient\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(16, 3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_8/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_9/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_10/TensorListStack:0' shape=(16, 1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_11/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_12/TensorListStack:0' shape=(16, 1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_13/TensorListStack:0' shape=(16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_14/TensorListStack:0' shape=(16, 1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_15/TensorListStack:0' shape=(16, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_8:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_9:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_10:0' shape=(1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_11:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_12:0' shape=(1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_13:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_14:0' shape=(1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_15:0' shape=(10,) dtype=float32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute the gradient\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "clipped_gradients: [<tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack/TensorListStack:0' shape=(16, 3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_1/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_2/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_3/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_4/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_5/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_6/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_7/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_8/TensorListStack:0' shape=(16, 3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_9/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_10/TensorListStack:0' shape=(16, 1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_11/TensorListStack:0' shape=(16, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_12/TensorListStack:0' shape=(16, 1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_13/TensorListStack:0' shape=(16, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_14/TensorListStack:0' shape=(16, 1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/map/TensorArrayV2Stack_15/TensorListStack:0' shape=(16, 10) dtype=float32>]\n",
      "[<tf.Tensor 'SGD/gradients/Sum:0' shape=(3, 3, 3, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_2:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_3:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_4:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_5:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_6:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_7:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_8:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_9:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_10:0' shape=(1, 1, 64, 64) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_11:0' shape=(64,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_12:0' shape=(1, 1, 64, 16) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_13:0' shape=(16,) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_14:0' shape=(1024, 10) dtype=float32>, <tf.Tensor 'SGD/gradients/Sum_15:0' shape=(10,) dtype=float32>]\n",
      "3125/3125 [==============================] - 92s 28ms/step - loss: 2.2980 - accuracy: 0.1293 - val_loss: 2.2665 - val_accuracy: 0.1850\n",
      "Epoch 2/40\n",
      "3125/3125 [==============================] - 88s 28ms/step - loss: 2.2652 - accuracy: 0.1585 - val_loss: 2.3023 - val_accuracy: 0.0992\n",
      "Epoch 3/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 2.2928 - accuracy: 0.1095 - val_loss: 2.2146 - val_accuracy: 0.1652\n",
      "Epoch 4/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 2.2510 - accuracy: 0.1457 - val_loss: 2.1657 - val_accuracy: 0.1841\n",
      "Epoch 5/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 2.1727 - accuracy: 0.1968 - val_loss: 2.1082 - val_accuracy: 0.2208\n",
      "Epoch 6/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 2.1143 - accuracy: 0.2227 - val_loss: 2.0122 - val_accuracy: 0.2668\n",
      "Epoch 7/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 2.0591 - accuracy: 0.2567 - val_loss: 2.0782 - val_accuracy: 0.2221\n",
      "Epoch 8/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 2.0132 - accuracy: 0.2716 - val_loss: 2.5135 - val_accuracy: 0.1759\n",
      "Epoch 9/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 2.0094 - accuracy: 0.2848 - val_loss: 1.7499 - val_accuracy: 0.3812\n",
      "Epoch 10/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 1.8768 - accuracy: 0.3272 - val_loss: 1.6820 - val_accuracy: 0.3980\n",
      "Epoch 11/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.7827 - accuracy: 0.3623 - val_loss: 1.6983 - val_accuracy: 0.3945\n",
      "Epoch 12/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.6786 - accuracy: 0.3979 - val_loss: 1.6334 - val_accuracy: 0.4182\n",
      "Epoch 13/40\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 1.6205 - accuracy: 0.4203 - val_loss: 1.5509 - val_accuracy: 0.4382\n",
      "Epoch 14/40\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 1.5611 - accuracy: 0.4420 - val_loss: 1.5081 - val_accuracy: 0.4602\n",
      "Epoch 15/40\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 1.5238 - accuracy: 0.4557 - val_loss: 1.4506 - val_accuracy: 0.4855\n",
      "Epoch 16/40\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 1.4768 - accuracy: 0.4708 - val_loss: 1.4202 - val_accuracy: 0.5007\n",
      "Epoch 17/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 1.4450 - accuracy: 0.4807 - val_loss: 1.3928 - val_accuracy: 0.5019\n",
      "Epoch 18/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 1.4523 - accuracy: 0.4841 - val_loss: 1.4085 - val_accuracy: 0.4973\n",
      "Epoch 19/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 1.4076 - accuracy: 0.4947 - val_loss: 1.3751 - val_accuracy: 0.5077\n",
      "Epoch 20/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.3823 - accuracy: 0.5038 - val_loss: 1.4623 - val_accuracy: 0.4712\n",
      "Epoch 21/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.3705 - accuracy: 0.5103 - val_loss: 1.3171 - val_accuracy: 0.5294\n",
      "Epoch 22/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.3498 - accuracy: 0.5208 - val_loss: 1.3154 - val_accuracy: 0.5271\n",
      "Epoch 23/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.3187 - accuracy: 0.5311 - val_loss: 1.2687 - val_accuracy: 0.5451\n",
      "Epoch 24/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 1.2791 - accuracy: 0.5435 - val_loss: 1.2755 - val_accuracy: 0.5385\n",
      "Epoch 25/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 1.2786 - accuracy: 0.5458 - val_loss: 1.2677 - val_accuracy: 0.5490\n",
      "Epoch 26/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 1.2495 - accuracy: 0.5536 - val_loss: 1.2302 - val_accuracy: 0.5568\n",
      "Epoch 27/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 1.2439 - accuracy: 0.5601 - val_loss: 1.1812 - val_accuracy: 0.5774\n",
      "Epoch 28/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 1.1929 - accuracy: 0.5731 - val_loss: 1.2556 - val_accuracy: 0.5571\n",
      "Epoch 29/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.1883 - accuracy: 0.5823 - val_loss: 1.2927 - val_accuracy: 0.5328\n",
      "Epoch 30/40\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 1.1778 - accuracy: 0.5814 - val_loss: 1.1624 - val_accuracy: 0.5899\n",
      "Epoch 31/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 1.1431 - accuracy: 0.5950 - val_loss: 1.1757 - val_accuracy: 0.5795\n",
      "Epoch 32/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 1.1322 - accuracy: 0.5992 - val_loss: 1.1091 - val_accuracy: 0.6077\n",
      "Epoch 33/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.0973 - accuracy: 0.6129 - val_loss: 1.1270 - val_accuracy: 0.6063\n",
      "Epoch 34/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 1.0727 - accuracy: 0.6230 - val_loss: 1.0982 - val_accuracy: 0.6130\n",
      "Epoch 35/40\n",
      "3125/3125 [==============================] - 86s 27ms/step - loss: 1.0457 - accuracy: 0.6308 - val_loss: 1.0724 - val_accuracy: 0.6188\n",
      "Epoch 36/40\n",
      "3125/3125 [==============================] - 84s 27ms/step - loss: 1.0432 - accuracy: 0.6320 - val_loss: 1.1603 - val_accuracy: 0.5945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 1.0376 - accuracy: 0.6355 - val_loss: 1.0804 - val_accuracy: 0.6199\n",
      "Epoch 38/40\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 1.0092 - accuracy: 0.6480 - val_loss: 1.0867 - val_accuracy: 0.6159\n",
      "Epoch 39/40\n",
      "3125/3125 [==============================] - 86s 28ms/step - loss: 0.9975 - accuracy: 0.6520 - val_loss: 1.1085 - val_accuracy: 0.6141\n",
      "Epoch 40/40\n",
      "3125/3125 [==============================] - 87s 28ms/step - loss: 0.9568 - accuracy: 0.6614 - val_loss: 1.0238 - val_accuracy: 0.6395\n",
      "Epoch 1/40\n",
      "49/49 [==============================] - 306s 4s/step - loss: 2.3112 - accuracy: 0.0991 - val_loss: 2.3029 - val_accuracy: 0.0958\n",
      "Epoch 2/40\n",
      "49/49 [==============================] - 20s 400ms/step - loss: 2.3032 - accuracy: 0.0975 - val_loss: 2.3029 - val_accuracy: 0.0987\n",
      "Epoch 3/40\n",
      "49/49 [==============================] - 20s 402ms/step - loss: 2.3029 - accuracy: 0.0994 - val_loss: 2.3028 - val_accuracy: 0.0985\n",
      "Epoch 4/40\n",
      "49/49 [==============================] - 19s 394ms/step - loss: 2.3028 - accuracy: 0.0998 - val_loss: 2.3026 - val_accuracy: 0.1009\n",
      "Epoch 5/40\n",
      "49/49 [==============================] - 19s 386ms/step - loss: 2.3028 - accuracy: 0.0955 - val_loss: 2.3025 - val_accuracy: 0.1031\n",
      "Epoch 6/40\n",
      "49/49 [==============================] - 19s 385ms/step - loss: 2.3026 - accuracy: 0.1026 - val_loss: 2.3026 - val_accuracy: 0.1010\n",
      "Epoch 7/40\n",
      "49/49 [==============================] - 19s 386ms/step - loss: 2.3026 - accuracy: 0.0989 - val_loss: 2.3027 - val_accuracy: 0.0999\n",
      "Epoch 8/40\n",
      "49/49 [==============================] - 19s 395ms/step - loss: 2.3027 - accuracy: 0.0980 - val_loss: 2.3027 - val_accuracy: 0.1027\n",
      "Epoch 9/40\n",
      "49/49 [==============================] - 19s 395ms/step - loss: 2.3026 - accuracy: 0.0986 - val_loss: 2.3026 - val_accuracy: 0.0994\n",
      "Epoch 10/40\n",
      "49/49 [==============================] - 19s 392ms/step - loss: 2.3026 - accuracy: 0.0990 - val_loss: 2.3026 - val_accuracy: 0.0998\n",
      "Epoch 11/40\n",
      "49/49 [==============================] - 19s 390ms/step - loss: 2.3026 - accuracy: 0.1007 - val_loss: 2.3026 - val_accuracy: 0.0996\n",
      "Epoch 12/40\n",
      "49/49 [==============================] - 19s 396ms/step - loss: 2.3026 - accuracy: 0.0982 - val_loss: 2.3025 - val_accuracy: 0.0998\n",
      "Epoch 13/40\n",
      "49/49 [==============================] - 19s 387ms/step - loss: 2.3026 - accuracy: 0.1022 - val_loss: 2.3025 - val_accuracy: 0.1034\n",
      "Epoch 14/40\n",
      "49/49 [==============================] - 19s 383ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.0960\n",
      "Epoch 15/40\n",
      "49/49 [==============================] - 19s 389ms/step - loss: 2.3026 - accuracy: 0.0954 - val_loss: 2.3026 - val_accuracy: 0.0966\n",
      "Epoch 16/40\n",
      "49/49 [==============================] - 19s 398ms/step - loss: 2.3026 - accuracy: 0.0976 - val_loss: 2.3026 - val_accuracy: 0.0946\n",
      "Epoch 17/40\n",
      "49/49 [==============================] - 19s 398ms/step - loss: 2.3026 - accuracy: 0.0995 - val_loss: 2.3025 - val_accuracy: 0.1060\n",
      "Epoch 18/40\n",
      "49/49 [==============================] - 18s 378ms/step - loss: 2.3026 - accuracy: 0.0990 - val_loss: 2.3026 - val_accuracy: 0.0943\n",
      "Epoch 19/40\n",
      "49/49 [==============================] - 19s 384ms/step - loss: 2.3026 - accuracy: 0.0985 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 20/40\n",
      "49/49 [==============================] - 19s 396ms/step - loss: 2.3026 - accuracy: 0.1004 - val_loss: 2.3026 - val_accuracy: 0.1004\n",
      "Epoch 21/40\n",
      "49/49 [==============================] - 19s 397ms/step - loss: 2.3026 - accuracy: 0.1004 - val_loss: 2.3025 - val_accuracy: 0.1015\n",
      "Epoch 22/40\n",
      "49/49 [==============================] - 19s 396ms/step - loss: 2.3026 - accuracy: 0.1018 - val_loss: 2.3025 - val_accuracy: 0.0996\n",
      "Epoch 23/40\n",
      "49/49 [==============================] - 19s 383ms/step - loss: 2.3026 - accuracy: 0.1010 - val_loss: 2.3026 - val_accuracy: 0.1017\n",
      "Epoch 24/40\n",
      "49/49 [==============================] - 19s 393ms/step - loss: 2.3026 - accuracy: 0.1010 - val_loss: 2.3026 - val_accuracy: 0.1001\n",
      "Epoch 25/40\n",
      "49/49 [==============================] - 19s 382ms/step - loss: 2.3027 - accuracy: 0.1005 - val_loss: 2.3026 - val_accuracy: 0.0993\n",
      "Epoch 26/40\n",
      "49/49 [==============================] - 19s 393ms/step - loss: 2.3026 - accuracy: 0.1017 - val_loss: 2.3025 - val_accuracy: 0.1005\n",
      "Epoch 27/40\n",
      "49/49 [==============================] - 19s 394ms/step - loss: 2.3026 - accuracy: 0.0990 - val_loss: 2.3026 - val_accuracy: 0.0997\n",
      "Epoch 28/40\n",
      "49/49 [==============================] - 19s 397ms/step - loss: 2.3027 - accuracy: 0.0987 - val_loss: 2.3026 - val_accuracy: 0.0991\n",
      "Epoch 29/40\n",
      "49/49 [==============================] - 19s 399ms/step - loss: 2.3026 - accuracy: 0.0996 - val_loss: 2.3026 - val_accuracy: 0.0980\n",
      "Epoch 30/40\n",
      "49/49 [==============================] - 19s 391ms/step - loss: 2.3026 - accuracy: 0.0988 - val_loss: 2.3026 - val_accuracy: 0.1008\n",
      "Epoch 31/40\n",
      "49/49 [==============================] - 19s 396ms/step - loss: 2.3026 - accuracy: 0.1010 - val_loss: 2.3026 - val_accuracy: 0.1031\n",
      "Epoch 32/40\n",
      "49/49 [==============================] - 19s 387ms/step - loss: 2.3026 - accuracy: 0.1010 - val_loss: 2.3026 - val_accuracy: 0.0956\n",
      "Epoch 33/40\n",
      "49/49 [==============================] - 19s 393ms/step - loss: 2.3026 - accuracy: 0.0996 - val_loss: 2.3026 - val_accuracy: 0.1002\n",
      "Epoch 34/40\n",
      "49/49 [==============================] - 19s 391ms/step - loss: 2.3026 - accuracy: 0.0988 - val_loss: 2.3026 - val_accuracy: 0.1003\n",
      "Epoch 35/40\n",
      "49/49 [==============================] - 19s 380ms/step - loss: 2.3026 - accuracy: 0.0956 - val_loss: 2.3026 - val_accuracy: 0.1020\n",
      "Epoch 36/40\n",
      "49/49 [==============================] - 19s 392ms/step - loss: 2.3026 - accuracy: 0.0987 - val_loss: 2.3026 - val_accuracy: 0.0968\n",
      "Epoch 37/40\n",
      "49/49 [==============================] - 19s 389ms/step - loss: 2.3026 - accuracy: 0.1010 - val_loss: 2.3026 - val_accuracy: 0.0939\n",
      "Epoch 38/40\n",
      "49/49 [==============================] - 19s 383ms/step - loss: 2.3026 - accuracy: 0.1009 - val_loss: 2.3026 - val_accuracy: 0.1024\n",
      "Epoch 39/40\n",
      "49/49 [==============================] - 19s 384ms/step - loss: 2.3026 - accuracy: 0.1012 - val_loss: 2.3025 - val_accuracy: 0.1029\n",
      "Epoch 40/40\n",
      "49/49 [==============================] - 19s 382ms/step - loss: 2.3026 - accuracy: 0.0981 - val_loss: 2.3026 - val_accuracy: 0.1004\n"
     ]
    }
   ],
   "source": [
    "# check the training accuracy\n",
    "dpsgd = True\n",
    "parameter_list = {}\n",
    "accuracies = {}\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+cifar10+tied_bias\", \"ALLnoise+cifar10+tied_bias\",\"cifar10+tied_bias\", \"cifar10+tied_bias+noise_input\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"cifar10+tied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=learning_rate)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sphinx+cifar10+tied_bias': [<tensorflow.python.keras.callbacks.History at 0x7f51681eff10>],\n",
       " 'ALLnoise+cifar10+tied_bias': [<tensorflow.python.keras.callbacks.History at 0x7f51604d41d0>],\n",
       " 'cifar10+tied_bias': [<tensorflow.python.keras.callbacks.History at 0x7f5468946a50>],\n",
       " 'cifar10+tied_bias+noise_input': [<tensorflow.python.keras.callbacks.History at 0x7f5468a4e710>]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_accuracies = dict()\n",
    "for key in accuracies.keys():   \n",
    "    processed_accuracies[key] = accuracies[key][0].history\n",
    "    \n",
    "import pickle\n",
    "file = open(\"dp_data/results/cifar10_accuracies_%.1f_%.1f_%.1f_%d\"%(learning_rate, noise_multiplier, l2_norm_clip, batch_size),\"wb\")\n",
    "pickle.dump(processed_accuracies, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8808c4021c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Label : skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_noise(x, clip, noise_multiplier):\n",
    "    def clip_features(x):\n",
    "        return tf.clip_by_global_norm([x], clip)[0][0]\n",
    "    clipped_features = tf.map_fn(clip_features, x)\n",
    "    # Add noise to summed gradients.\n",
    "    noise_stddev = clip * noise_multiplier\n",
    "    noise = tf.random.normal(tf.shape(input=clipped_features), stddev=noise_stddev)\n",
    "    return clipped_features + noise\n",
    "\n",
    "noisy_train_data = tf.map_fn(lambda x: clip_noise(x, l2_norm_clip, noise_multiplier), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies = dict()\n",
    "for key in accuracies.keys():   \n",
    "    processed_accuracies[key] = accuracies[key][0].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(processed_accuracies['sphinx+mnist+untied_bias']['accuracy'])\n",
    "plt.plot(processed_accuracies['sphinx+mnist+untied_bias']['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"dp_data/results/accuracies_%.1f_%.1f_%.1f_%d\"%(learning_rate, noise_multiplier, l2_norm_clip, batch_size),\"wb\")\n",
    "pickle.dump(processed_accuracies, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the step-epsilon curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "try:\n",
    "    with open(\"dp_data/noise_epsilon_step\") as f:\n",
    "        epsilons = pickle.load(f)\n",
    "except:\n",
    "    training_data_size = 60000\n",
    "    max_step = 25000\n",
    "    epsilons = [[] for i in range(10)]\n",
    "    for noise_multiplier in range(0, 10, 1):\n",
    "        print(\"noise_multiplier:\", noise_multiplier)\n",
    "        for step in range(1, max_step, 10):\n",
    "            epsilons[noise_multiplier].append(compute_epsilon(step, batch_size/training_data_size, noise_multiplier))\n",
    "    file = open(\"dp_data/noise_epsilon_step_%d\"%batch_size,\"wb\")\n",
    "    pickle.dump(epsilons, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tied Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tied bias\n",
    "#for model_type in [\"mnist\", \"mnist+tied_bias\", \"mnist+untied_bias\"]:\n",
    "for model_type in [\"sphinx+mnist+tied_bias\", \"ALLnoise+mnist+tied_bias\",\"mnist+tied_bias\"]:\n",
    "    file_name = model_type + \"_accuracy\"\n",
    "    if model_type == \"mnist+tied_bias+noise_input\":\n",
    "        dpsgd = False\n",
    "    else:\n",
    "        dpsgd = True\n",
    "    model = build_models(model_type)\n",
    "    if dpsgd:\n",
    "        optimizer = FixedDPKerasSGDOptimizer(\n",
    "            batch_size = batch_size,\n",
    "            num_parameters = num_parameters,\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            var_list = model.trainable_variables,\n",
    "            #num_microbatches=batch_size//microbatch_size,\n",
    "            microbatch_size = microbatch_size,\n",
    "            learning_rate=learning_rate)\n",
    "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile model with Keras\n",
    "    checkpoint_filepath = \"./dp_data/\"+file_name\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_accuracy',\n",
    "      mode='max',\n",
    "      save_best_only=True)\n",
    "    #early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    ##    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    #    mode='auto', baseline=None, restore_best_weights=True\n",
    "    #)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    # Train model with Keras\n",
    "    if dpsgd:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=microbatch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    else:\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(test_data, test_labels),\n",
    "                batch_size=batch_size, \n",
    "                callbacks = [model_checkpoint_callback], workers=1) # , early_stopping_callback\n",
    "    \n",
    "    #evaluated_result = model.evaluate(\n",
    "    #    x=test_data, y=test_labels, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
    "    #    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    #    return_dict=True)\n",
    "    if model_type not in accuracies.keys():\n",
    "        accuracies[model_type] = []\n",
    "    accuracies[model_type].append(history)\n",
    "    #accuracies[model_type].append(evaluated_result[\"accuracy\"])\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "#if dpsgd:\n",
    "#    eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "#    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "#else:\n",
    "#    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "for key in parameter_list.keys():\n",
    "    line1, = ax.plot(parameter_list[key],accuracies[key],  label=key)\n",
    "    line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee\n",
    "import pickle\n",
    "result = open(\"result\",\"wb\")\n",
    "pickle.dump(accuracies, result)\n",
    "pickle.dump(parameter_list, result)\n",
    "result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(\"result\", \"rb\")\n",
    "accuracies = pickle.load(result)\n",
    "parameter_list = pickle.load(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "line1, = ax.plot(parameter_list[\"linear\"],accuracies[\"linear\"],  label='Linear')\n",
    "line1.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "line2, = ax.plot(parameter_list[\"bias\"],  accuracies[\"bias\"], label='Bias')\n",
    "line2.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "a = [tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32), tf.constant([3., 4, 5], shape=(3,), dtype=tf.float32), tf.constant([2., 3, 4.], shape=(3,), dtype=tf.float32)]\n",
    "b = tf.ones_like(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linalg.global_norm(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_global_norm(a, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
